{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"QN_OBdCcdsFo"},"source":["# Exercise 2: Shape Classification and Segmentation\n","\n","**Submission Deadline**: 23.11.2022, 23:55\n","\n","In this exercise, we will dive into Machine Learning on 3D shapes by taking a look at shape classification and segmentation."]},{"cell_type":"markdown","metadata":{"id":"D_D_CjQudsFu"},"source":["## 2.0. Running this notebook\n","We recommend running this notebook on a CUDA compatible local gpu. You can also run training on cpu, it will just take longer.\n","\n","You have three options for running this exercise on a GPU, choose one of them and start the exercise below in section \"Imports\":\n","1. Locally on your own GPU\n","2. On our dedicated compute cluster\n","3. On Google Colab\n","\n","We describe every option in more detail below:"]},{"cell_type":"markdown","metadata":{"id":"V3AMVuU3dsFw"},"source":["---\n","\n","### (a) Local Execution\n","\n","If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`.\n","\n","In case you are working with a RTX 3000-series GPU, you need to install a patched version of pytorch:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rulBdpUTdsFy","outputId":"f9685700-0896-47ce-aadc-d3971de519c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n","Requirement already satisfied: torch in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (1.13.0)\n","Requirement already satisfied: torchvision in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (0.14.0)\n","Requirement already satisfied: typing-extensions in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from torch) (4.4.0)\n","Requirement already satisfied: numpy in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from torchvision) (1.23.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from torchvision) (9.3.0)\n","Requirement already satisfied: requests in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from torchvision) (2.28.1)\n","Requirement already satisfied: charset-normalizer<3,>=2 in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from requests->torchvision) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from requests->torchvision) (2022.9.24)\n","Requirement already satisfied: idna<4,>=2.5 in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\tumsemester 2\\machine learning for 3d geometry\\exercises\\.env\\lib\\site-packages (from requests->torchvision) (1.26.12)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 21.2.3; however, version 22.3.1 is available.\n","You should consider upgrading via the 'D:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"]}],"source":["!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"]},{"cell_type":"markdown","metadata":{"id":"FE6q8V_UdsF0"},"source":["### (b) Compute Cluster\n","\n","We provide access to a small compute cluster for the exercises and projects, consisting of a login node and 4 compute nodes with one dedicated RTX 3090 GPU each.\n","Please send us a short email with your name and preferred username so we can add you as a user.\n","\n","We uploaded a PDF to Moodle with detailed information on how to access and use the cluster.\n","\n","Since the cluster contains RTX 3000-series GPUs, you will need to install a patched version of pytorch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88gFGZ8edsF1"},"outputs":[],"source":["!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"]},{"cell_type":"markdown","metadata":{"id":"JvadP165dsF3"},"source":["### (c) Google Colab\n","\n","If you don't have access to a GPU and don't want to use our cluster, you can also use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n","What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n","\n","In case you're using Google Colab, you can upload the exercise folder (containing `exercise_2.ipynb`, directory `exercise_2` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n","Additionally you'd need to open the notebook `exercise_2.ipynb` in Colab using `File > Open Notebook > Upload`.\n","\n","Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":123365,"status":"ok","timestamp":1669154451208,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"rXY0oWM5dsF5","outputId":"25e5bb91-e615-4e53-ab41-3f257ed3e061","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Installing requirements\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jupyter>=1.0.0\n","  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n","Collecting K3D>=2.9.4\n","  Downloading k3d-2.14.5-py2.py3-none-any.whl (15.1 MB)\n","\u001b[K     |████████████████████████████████| 15.1 MB 243 kB/s \n","\u001b[?25hCollecting matplotlib>=3.4.1\n","  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n","\u001b[K     |████████████████████████████████| 11.2 MB 62.7 MB/s \n","\u001b[?25hCollecting trimesh>=3.9.14\n","  Downloading trimesh-3.16.4-py3-none-any.whl (663 kB)\n","\u001b[K     |████████████████████████████████| 663 kB 63.5 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.12.1+cu113)\n","Collecting pytorch-lightning>=1.2.8\n","  Downloading pytorch_lightning-1.8.2-py3-none-any.whl (798 kB)\n","\u001b[K     |████████████████████████████████| 798 kB 71.9 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.18.3)\n","Collecting pyrender>=0.1.43\n","  Downloading pyrender-0.1.45-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 63.2 MB/s \n","\u001b[?25hCollecting moviepy>=1.0.3\n","  Downloading moviepy-1.0.3.tar.gz (388 kB)\n","\u001b[K     |████████████████████████████████| 388 kB 72.8 MB/s \n","\u001b[?25hCollecting pillow>=7.2.0\n","  Downloading Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 60.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (4.64.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.7.3)\n","Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.21.6)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 1)) (5.3.4)\n","Collecting qtconsole\n","  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n","\u001b[K     |████████████████████████████████| 121 kB 68.9 MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 1)) (7.7.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 1)) (5.7.16)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 1)) (5.6.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 1)) (6.1.0)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from K3D>=2.9.4->-r requirements.txt (line 2)) (1.0.4)\n","Collecting traittypes\n","  Downloading traittypes-0.2.1-py2.py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from K3D>=2.9.4->-r requirements.txt (line 2)) (5.1.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.1->-r requirements.txt (line 3)) (21.3)\n","Collecting fonttools>=4.22.0\n","  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n","\u001b[K     |████████████████████████████████| 965 kB 67.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.1->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.1->-r requirements.txt (line 3)) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.1->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.1->-r requirements.txt (line 3)) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->-r requirements.txt (line 5)) (4.1.1)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (2.9.1)\n","Collecting lightning-utilities==0.3.*\n","  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (6.0)\n","Collecting torchmetrics>=0.7.0\n","  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 77.0 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (2022.11.0)\n","Collecting fire\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.18.1->-r requirements.txt (line 7)) (1.3.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.18.1->-r requirements.txt (line 7)) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.18.1->-r requirements.txt (line 7)) (2021.11.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.18.1->-r requirements.txt (line 7)) (2.6.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyrender>=0.1.43->-r requirements.txt (line 8)) (1.15.0)\n","Collecting freetype-py\n","  Downloading freetype_py-2.3.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (978 kB)\n","\u001b[K     |████████████████████████████████| 978 kB 64.4 MB/s \n","\u001b[?25hCollecting pyglet>=1.4.10\n","  Downloading pyglet-2.0.0-py3-none-any.whl (966 kB)\n","\u001b[K     |████████████████████████████████| 966 kB 71.8 MB/s \n","\u001b[?25hCollecting PyOpenGL==3.1.0\n","  Downloading PyOpenGL-3.1.0.zip (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 65.2 MB/s \n","\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy>=1.0.3->-r requirements.txt (line 9)) (4.4.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from moviepy>=1.0.3->-r requirements.txt (line 9)) (2.23.0)\n","Collecting proglog<=1.0.0\n","  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n","Collecting imageio_ffmpeg>=0.2.0\n","  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[K     |████████████████████████████████| 26.9 MB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (3.8.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (22.1.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.8.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (4.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.3->-r requirements.txt (line 9)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.3->-r requirements.txt (line 9)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.3->-r requirements.txt (line 9)) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.3->-r requirements.txt (line 9)) (1.24.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.3.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (3.19.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (0.38.4)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.50.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (3.4.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (2.14.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (0.6.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (5.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (4.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (3.10.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (3.2.2)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning>=1.2.8->-r requirements.txt (line 6)) (2.1.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (6.1.12)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (7.9.0)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (6.0.4)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (2.0.10)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.7.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (2.6.1)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 54.5 MB/s \n","\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.8.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.2.5)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 1)) (3.0.3)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 1)) (3.6.1)\n","Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (2.11.3)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (5.7.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.13.3)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.15.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (23.2.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (1.8.0)\n","Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (4.11.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.8.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.6.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (5.0.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (2.16.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (4.3.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.19.2)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (5.10.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 1)) (0.5.1)\n","Collecting qtpy>=2.0.1\n","  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 2.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyOpenGL, moviepy, fire\n","  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyOpenGL: filename=PyOpenGL-3.1.0-py3-none-any.whl size=1745210 sha256=5700d869bf4f714f7bc5f763b6f0fe87a1012645b5fb914bca909f5115ca67ef\n","  Stored in directory: /root/.cache/pip/wheels/c6/83/cb/af51a0c06c33d08537b941bbfc87469e8a3c68d05f77a6a212\n","  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110742 sha256=0f8289d223ddb295a415022a5da22966b195043c9fa06977b26edf88940bd9f5\n","  Stored in directory: /root/.cache/pip/wheels/56/dc/2b/9cd600d483c04af3353d66623056fc03faed76b7518faae4df\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=6334eab67aa9b77cba85f850b2da3c00134bb52c666e6f800ffe714ed1edf7aa\n","  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n","Successfully built PyOpenGL moviepy fire\n","Installing collected packages: jedi, qtpy, pillow, fonttools, fire, trimesh, traittypes, torchmetrics, qtconsole, PyOpenGL, pyglet, proglog, matplotlib, lightning-utilities, imageio-ffmpeg, freetype-py, pytorch-lightning, pyrender, moviepy, K3D, jupyter\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: PyOpenGL\n","    Found existing installation: PyOpenGL 3.1.6\n","    Uninstalling PyOpenGL-3.1.6:\n","      Successfully uninstalled PyOpenGL-3.1.6\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Attempting uninstall: moviepy\n","    Found existing installation: moviepy 0.2.3.5\n","    Uninstalling moviepy-0.2.3.5:\n","      Successfully uninstalled moviepy-0.2.3.5\n","Successfully installed K3D-2.14.5 PyOpenGL-3.1.0 fire-0.4.0 fonttools-4.38.0 freetype-py-2.3.0 imageio-ffmpeg-0.4.7 jedi-0.18.2 jupyter-1.0.0 lightning-utilities-0.3.0 matplotlib-3.5.3 moviepy-1.0.3 pillow-9.3.0 proglog-0.1.10 pyglet-2.0.0 pyrender-0.1.45 pytorch-lightning-1.8.2 qtconsole-5.4.0 qtpy-2.3.0 torchmetrics-0.10.3 traittypes-0.2.1 trimesh-3.16.4\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","matplotlib","mpl_toolkits"]}}},"metadata":{},"output_type":"display_data"}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# We assume you uploaded the exercise folder in root Google Drive folder\n","\n","!cp -r \"/content/drive/MyDrive/Machine Learning for 3D Geometry/Exercise_2\" \"3d-machine-learning/\"\n","os.chdir('/content/3d-machine-learning/')\n","print('Installing requirements')\n","!pip install -r requirements.txt\n","\n","# Make sure you restart runtime when directed by Colab"]},{"cell_type":"markdown","metadata":{"id":"4Onc9u7PdsF7"},"source":["Run this cell after restarting your colab runtime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2469,"status":"ok","timestamp":1669154472468,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"dUldj9wwdsF8","outputId":"ca4c8f74-ad44-40a1-e18e-9a2aa723d920","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import os\n","import sys\n","import torch\n","os.chdir('/content/3d-machine-learning/')\n","sys.path.insert(1, \"/content/3d-machine-learning/\")\n","print('CUDA availability:', torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"am1vNy2CdsF9"},"source":["---\n","\n","### Imports\n","\n","The following imports should work regardless of whether you are using Colab or local execution."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1593,"status":"ok","timestamp":1669154490131,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"1Ffp1cOMdsF-"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","from pathlib import Path\n","import numpy as np\n","import matplotlib as plt\n","import k3d\n","import trimesh\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"pbgYBlYjdsF-"},"source":["Use the next cell to test whether a GPU was detected by pytorch."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669154492972,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"loWhnLhWdsF_","outputId":"4ccd09bd-e598-4509-d759-1f7ec25a75e0"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"Gh8OqVu0dsF_"},"source":["## 2.1. ShapeNet Terms of Use\n","\n","We provide pre-processed shapes from the [ShapeNet](//shapenet.org) database for this exercise. ShapeNet is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes and is a collaborative effort between researchers at Princeton, Stanford and TTIC.\n","\n","<img src=\"exercise_2/images/shapenet.png\" alt=\"shapenet\" style=\"width: 512px;\"/>\n","\n","In order to be able to use the data, we ask you to read and agree to their Terms of Use as stated below (this is a requirement for passing this exercise):\n","\n","1. Researcher shall use the Database only for non-commercial research and educational purposes.\n","2. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n","3. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify Princeton University and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted 3D models that he or she may create from the Database.\n","4. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n","5. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n","6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n","7. The law of the State of New Jersey shall apply to all disputes under this agreement.\n","\n","To agree, simply type `I agree to the Terms of Use` in the next code cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSfSy-tAdsGA"},"outputs":[],"source":["# I agree to the Terms of Use"]},{"cell_type":"markdown","metadata":{"id":"GS3Wp-RgdsGB"},"source":["## 2.2. A simple 3D CNN with pytorch\n","\n","Here, we will create a very simple 3D Convolutional Neural Network on some toy data. This is meant as a quick introduction into the pytorch framework if you haven't used it yet. It will cover everything you need to know for the following parts of the exercise; if you want to go into a bit more detail about the framework, a good place to start is the official [pytorch quickstart tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html).\n","\n","A very useful resource if you want to know more about a certain class or function are the [official docs](https://pytorch.org/docs/stable/index.html). Take a minute to look at the documentation of a [Tensor](https://pytorch.org/docs/stable/tensors.html?highlight=tensor#torch.Tensor) which is the data structure pytorch uses throughout their APIs. It is comparable to a numpy array, with added functionality such as the ability to move seamlessly between cpu host memory and cuda device memory or the integrated autograd functionality for automatic differentiation. Most importantly, you can create a tensor from a numpy array with `torch.from_numpy(array)` and the other way around with `array = tensor.numpy()`.\n","\n","Our objective with this 3D CNN is to simply classify if a given SDF is containing a sphere or a torus. The data is generated the same way we did it in the last exercise (we randomize the shape parameters a bit so the task is not too trivial). Each such sample has a scalar label associated with it, either 0 or 1, depending on the shape it contains.\n","\n","All of the implementation in this part will take place in `exercise_2/simple_nn.py`. This makes sense for now but note that we would usually spread things over multiple different files like in the following exercise parts.\n","\n","The important bits we need for every training in pytorch are:\n","1. The **dataset** implementation. This is a class responsible for providing the data used by the training procedure sample-by-sample. In its simplest form, it just loads data samples (eg. point clouds or voxel grids) from disk. However, it is usually also used to transform raw data from disk into the correct format and to apply various kinds of augmentations.\n","2. The network definition (\"**model**\") that we want to train. It specifies the network structure (number and type of layers, activation functions, normalization layers etc.) as well as how exactly input data is processed.\n","3. The **loss function**: For classification, this is usually a Cross Entropy Loss; we will also see reconstruction losses like l1 and l2 in exercise 3.\n","4. The **optimizer**: Usually chosen from a set of pre-defined classes like SGD or ADAM.\n","5. The **training loop**: For each batch of data, it does the following: Loads data from the dataloader, passes it through the model (\"forward pass\"), computes the loss, calculates the gradients (\"backward pass\"), and finally, adjusts the network weights using the optimizer.\n","\n","Let's walk through everything step-by-step:"]},{"cell_type":"markdown","metadata":{"id":"ZMfR-e7PdsGB"},"source":["### (a) Dataset\n","\n","We start by implementing the data source, which is called `Dataset` in pytorch. Take a look at the `SimpleDataset` class, it contains everything we have to implement to make it work:\n","1. The `__init__` function that takes paramters and prepares the dataset by setting paths etc. Here, the one thing you always have to do is to load at least a list of samples you want to use - they don't have to be loaded from disk yet, but you need to know which and how many samples there are. This usually also depends on the current split - you use a different set of samples for training than for validation or testing.\n","2. The `__len__` function that returns the total number of samples. In most cases, this will simply be the length of the list of samples you prepared in `__init__`.\n","3. The `__getitem__` function. This is where the actual dataloading takes place. The function gets an index between 0 and the length you returned with `__len__`. Your job is then to return some data corresponding to the index. You do not need to worry about putting data onto the GPU here yet; instead, you load the data and return a tuple containing numpy arrays and other data like sample ids you might need in your training loop. Overall, you should take the index parameter in this function, find the sample id it belongs to, load that data, and then return it in a format that can be used in the training loop.\n","\n","Note that it is also fine to load all data into a list in the `__init__` function for small datasets like our toy dataset in this exercise part. You would then just index this pre-loaded data in your `__getitem__` function.\n","\n","Add and implement functions `__init__`, `__len__`, and `__getitem__` in class `SimpleDataset`. \n","\n","First, implement `__init__`: It takes a single parameter (called `split`) that determines if we are using train or val split at the moment. Based on that, it should generate toy data: 4096 samples if the split is train and 1024 if it is val. Use `generate_toy_data` from `exercise_2/util/toy_data.py` for this which generates two numpy arrays: The first one containing sphere and torus SDFs generated with randomized parameters and the second one classification labels, a 0 if the corresponding SDF in the first array is a sphere and 1 if it is a torus.\n","\n","Then, implement `__len__` by simply returning the length of the data you generated in `__init__`.\n","\n","Lastly, implement `__getitem__` which takes an integer index and returns a tuple: A numpy array containing the input SDF volume as generated by `generate_toy_data` before and a scalar value that represents the target class label for this volume.\n","\n","Note: You have to add an additional dimension of size 1 to the input volume you return in `__getitem__` to make everything work later on, i.e. instead of returning a numpy array of shape (32, 32, 32), you should return an array of shape (1, 32, 32, 32).\n","\n","Test your implementation with the checks below:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4226,"status":"ok","timestamp":1669154504757,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"dpRQ4JX7dsGD","outputId":"4220b40f-22a4-4a5e-b5cc-e193a613efaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating toy data ...\n","Generating toy data ...\n","Length of train set: 4096\n","Length of val set: 1024\n","(1, 32, 32, 32)\n","Class = 0\n","(1, 32, 32, 32)\n","Class = 1\n"]}],"source":["from exercise_2.simple_nn import SimpleDataset\n","\n","# Create datasets with train and val splits\n","train_dataset = SimpleDataset('train')\n","val_dataset = SimpleDataset('val')\n","\n","# Test lengths\n","print(f'Length of train set: {len(train_dataset)}')  # expected output: 4096\n","print(f'Length of val set: {len(val_dataset)}')  # expected output: 1024\n","\n","# Get sample at index 0\n","train_sample = train_dataset[0]\n","print(train_sample[0].shape)  # Expected output (1, 32, 32, 32) (the leading 1 is important for later)\n","print(f\"Class = {train_sample[1]}\")  # Expected output: Scalar value 0\n","val_sample = val_dataset[-1]\n","print(val_sample[0].shape)  # Expected output (1, 32, 32, 32) (the leading 1 is important for later)\n","print(f\"Class = {val_sample[1]}\")  # Expected output: Scalar value 1"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1669154504758,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"QjInCSfXdsGE","outputId":"ee709d9f-e32f-4431-9a1a-5121d35f3fff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating SDF visualization for 32^3 grid ...\n","Exported to d:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\toy_data.ply\n"]}],"source":["from exercise_2.util.visualization import visualize_sdf\n","\n","train_sample = train_dataset[0]\n","visualize_sdf(train_sample[0].squeeze(0), filename=Path.cwd() / 'toy_data.ply')"]},{"cell_type":"markdown","metadata":{"id":"ntzoQ9rzdsGF"},"source":["### (b) Model\n","The model is defined in class `SimpleModel`. Two functions are important here: `__init__` which sets up the architecture by instantiating layers with the correct parameters and adapting them to possible input parameters and `forward` which takes an input tensor (we call it `x` here). The output of `forward` is another tensor that contains the result of the network - in our case, this is a vector of logits which will then be used for classification. It could also be a volume of labels if you do segmentation or a volume of the same size as the input if you do reconstruction.\n","\n","Analogous to `forward`, you could also define a custom `backward` function that describes how to calculate the gradients for your model. However, this is not necessary for most architectures as pytorch's autograd implementation takes care of all of that.\n","\n","For now, we will implement a very simple model: We stack together three 3D Convolution layers and have a fully connected layer at the end for classification. This is a schematic for the overall model structure:\n","<img src=\"exercise_2/images/simplenn.png\" alt=\"simplenn_architecture\" style=\"width: 512px;\"/>\n","\n","Each layer starts with a 3d convolution, followed by a batch normalization layer and a ReLU activation function. We left the implementation of the first layer in there; your task is to fill out the missing parts in `__init__` for layers 2 and 3 (use the same parameters as for the first convolution: kernel size 4, stride 3, padding 1). Each convolution should double the number of feature channels, such that you get a volume of size 16 x 1 x 1 x 1 after layer 3. You also need to define the ReLU and the fully connected layer (for this, use `torch.nn.Linear` to reduce the number of dimensions from 16 to 2).\n","\n","In `forward`, you can repeat the first line to move the input tensor through all three convolutional layers. Then, reshape the resulting tensor to dimension batchsize x 16 (the batch size is always the first dimension) and apply the linear layer to it. Return the result as-is.\n","\n","Hint: The ease of debugging is one of the main reasons why pytorch is preferred in research over alternatives like tensorflow. You can set breakpoints in the `forward` function and watch as the input tensor gets modified by each layer to find bugs or things like wrongly calculated convolution parameters.\n","\n","Use the following sanity checks to verify your model:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1669154509716,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"leYO1-vtdsGF","outputId":"363654d4-be5d-4173-a885-eb364bc5e491"},"outputs":[{"name":"stdout","output_type":"stream","text":["  | Name  | Type        | Params\n","--------------------------------------\n","0 | conv1 | Conv3d      | 260   \n","1 | bn1   | BatchNorm3d | 8     \n","2 | conv2 | Conv3d      | 2056  \n","3 | bn2   | BatchNorm3d | 16    \n","4 | conv3 | Conv3d      | 8208  \n","5 | bn3   | BatchNorm3d | 32    \n","6 | cl    | Linear      | 34    \n","7 | relu  | ReLU        | 0     \n","8 | TOTAL | SimpleModel | 10614 \n","Output tensor shape:  torch.Size([32, 2])\n"]}],"source":["from exercise_2.simple_nn import SimpleModel\n","from exercise_2.util.model import summarize_model\n","\n","simple_nn = SimpleModel()\n","print(summarize_model(simple_nn))  # Expected: Rows 0-8 and TOTAL = 10614\n","\n","input_tensor = torch.randn(32, 1, 32, 32, 32)\n","predictions = simple_nn(input_tensor)\n","\n","print('Output tensor shape: ', predictions.shape)  # Expected: torch.Size([32, 2])"]},{"cell_type":"markdown","metadata":{"id":"uLcgiOXddsGG"},"source":["### (c) Training"]},{"cell_type":"markdown","metadata":{"id":"4fiOL4N3dsGH"},"source":["We already laid out most of the code structure you need to start training once you have your dataset and model defined. This code usually does not change much between projects; this is why there exist many different libraries to simplify it even more (you can take a look at pytorch lightning for example if you are interested). We will stick to the standard pytorch way of doing things for these exercises.\n","\n","The most important things left to do are:\n","1. Define the data loaders. They take the samples you provide in your dataset implementation and take care of loading multiple samples in parallel, shuffling the dataset, and combining samples into batches.\n","2. Define a loss function. For classification, this is usually `torch.nn.CrossEntropyLoss`\n","3. Instantiate the optimizer. Mostly, this will be ADAM (`torch.optim.Adam`) unless you have a good reason to use another one.\n","4. Implement the training loop: Get a batch of data from the data loader, move it to the GPU, perform a forward pass, compute the loss, calculate gradients in the backward pass, adjust weights in an optimizer step, repeat.\n","\n","Note: You have to move some stuff to the correct compute device (usually the GPU) by calling `.to(device)`: The model and loss function as well as the data you get from each batch.\n","\n","Take a look at the structure of `main` since you will also use it for the next exercise parts and fill in code in `train` at the blanks marked with TODO. Then, start the training below. For this exercise, we don't care too much about the results since it is a very easy classification task anyways. Your model should be able to get to > 98% validation accuracy, though. You can stop the training once that is the case."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97555,"status":"ok","timestamp":1669154611902,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"RvI3xSxQdsGH","outputId":"1af09284-c808-4bcd-c206-10f671ed82a3","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n","Generating toy data ...\n","Generating toy data ...\n","[000/00024] train_loss: 0.467\n","[000/00049] train_loss: 0.327\n","[000/00074] train_loss: 0.240\n","[000/00099] train_loss: 0.171\n","[000/00099] val_loss: 0.174, val_accuracy: 96.289%\n","[000/00124] train_loss: 0.171\n","[001/00021] train_loss: 0.123\n","[001/00046] train_loss: 0.126\n","[001/00071] train_loss: 0.114\n","[001/00071] val_loss: 0.167, val_accuracy: 95.508%\n","[001/00096] train_loss: 0.121\n","[001/00121] train_loss: 0.088\n","[002/00018] train_loss: 0.069\n","[002/00043] train_loss: 0.105\n","[002/00043] val_loss: 0.260, val_accuracy: 89.551%\n","[002/00068] train_loss: 0.089\n","[002/00093] train_loss: 0.092\n","[002/00118] train_loss: 0.086\n","[003/00015] train_loss: 0.090\n","[003/00015] val_loss: 0.077, val_accuracy: 97.754%\n","[003/00040] train_loss: 0.086\n","[003/00065] train_loss: 0.069\n","[003/00090] train_loss: 0.062\n","[003/00115] train_loss: 0.072\n","[003/00115] val_loss: 0.170, val_accuracy: 93.262%\n","[004/00012] train_loss: 0.063\n","[004/00037] train_loss: 0.090\n","[004/00062] train_loss: 0.089\n","[004/00087] train_loss: 0.094\n","[004/00087] val_loss: 0.061, val_accuracy: 98.438%\n","[004/00112] train_loss: 0.100\n","[005/00009] train_loss: 0.029\n","[005/00034] train_loss: 0.070\n","[005/00059] train_loss: 0.111\n","[005/00059] val_loss: 0.097, val_accuracy: 98.926%\n","[005/00084] train_loss: 0.137\n","[005/00109] train_loss: 0.089\n","[006/00006] train_loss: 0.047\n","[006/00031] train_loss: 0.104\n","[006/00031] val_loss: 0.077, val_accuracy: 98.242%\n","[006/00056] train_loss: 0.101\n","[006/00081] train_loss: 0.056\n","[006/00106] train_loss: 0.072\n","[007/00003] train_loss: 0.008\n","[007/00003] val_loss: 0.059, val_accuracy: 99.023%\n","[007/00028] train_loss: 0.058\n","[007/00053] train_loss: 0.092\n","[007/00078] train_loss: 0.095\n","[007/00103] train_loss: 0.063\n","[007/00103] val_loss: 0.041, val_accuracy: 99.023%\n","[008/00000] train_loss: 0.006\n","[008/00025] train_loss: 0.101\n","[008/00050] train_loss: 0.056\n","[008/00075] train_loss: 0.075\n","[008/00075] val_loss: 0.046, val_accuracy: 98.926%\n","[008/00100] train_loss: 0.044\n","[008/00125] train_loss: 0.041\n","[009/00022] train_loss: 0.052\n","[009/00047] train_loss: 0.040\n","[009/00047] val_loss: 0.037, val_accuracy: 99.121%\n","[009/00072] train_loss: 0.040\n","[009/00097] train_loss: 0.057\n","[009/00122] train_loss: 0.069\n","[010/00019] train_loss: 0.050\n","[010/00019] val_loss: 0.057, val_accuracy: 99.219%\n","[010/00044] train_loss: 0.057\n","[010/00069] train_loss: 0.044\n","[010/00094] train_loss: 0.064\n","[010/00119] train_loss: 0.047\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [6], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexercise_2\u001b[39;00m \u001b[39mimport\u001b[39;00m simple_nn\n\u001b[0;32m      3\u001b[0m config\u001b[39m=\u001b[39m{\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m2_2_simple_nn\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvalidate_every_n\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m\n\u001b[0;32m     12\u001b[0m }\n\u001b[1;32m---> 14\u001b[0m simple_nn\u001b[39m.\u001b[39;49mmain(config)  \u001b[39m# should have val_accuracy > 99%\u001b[39;00m\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\simple_nn.py:178\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    174\u001b[0m Path(\n\u001b[0;32m    175\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexercise_2/runs/\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m\"\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m train(model, train_dataloader, val_dataloader, device, config)\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\simple_nn.py:106\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, device, config)\u001b[0m\n\u001b[0;32m    104\u001b[0m total, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m    105\u001b[0m loss_val \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m--> 106\u001b[0m \u001b[39mfor\u001b[39;00m batch_val \u001b[39min\u001b[39;00m val_dataloader:\n\u001b[0;32m    107\u001b[0m     input_data, target_labels \u001b[39m=\u001b[39m batch_val\n\u001b[0;32m    108\u001b[0m     \u001b[39m# TODO Move input_data and target_labels to device\u001b[39;00m\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from exercise_2 import simple_nn\n","\n","config={\n","    'experiment_name': '2_2_simple_nn',\n","    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.001,\n","    'max_epochs': 25,\n","    'print_every_n': 25,\n","    'validate_every_n': 100\n","}\n","\n","simple_nn.main(config)  # should have val_accuracy > 99%"]},{"cell_type":"markdown","metadata":{"id":"TUwHL3-RdsGI"},"source":["That's it! In the following parts, we will now move on to more complicated problems and more involved network models."]},{"cell_type":"markdown","metadata":{"id":"ZslJPNYAdsGJ"},"source":["## 2.3. Shape Classification using 3DCNN\n","\n","### (a) Download and extract voxelized ShapeNet training data"]},{"cell_type":"markdown","metadata":{"id":"visXCpyPdsGJ"},"source":["Each folder in the `exercise_2/data/ShapeNetVox32` directory represents a shape category represented by a number, e.g. `02691156`.\n","We provide the mapping between these numbers and the corresponding names in `exercise_2/data/shape_info.json`. Each of these shape category folders contains a number of shapes\n","represented as voxels and stored in a `binvox` format.\n","\n","```\n","# contents of exercise_2/data/ShapeNetVox32\n","\n","02691156/                                   # Shape category folder with all it's shapes\n","    ├── 1a04e3eab45ca15dd86060f189eb133/    # A single shape of the category\n","        ├── model.binvox                    # Voxel representation of the shape in binvox format\n","    ├── 1a6ad7a24bb89733f412783097373bdc/   # Another shape of the category\n","    ├── 1a9b552befd6306cc8f2d5fe7449af61/\n","    ├── :                                   # And so on ...\n","    ├── :\n","02828884/                                   # Another shape category folder\n","02933112/                                   # In total you should have 13 shape category folders\n",":\n",":\n","```"]},{"cell_type":"markdown","metadata":{"id":"hX_ZwCYY81Of"},"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1669154740386,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"Qw8G2LS2dsGW"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading ...\n","Extracting ...\n"]},{"name":"stderr","output_type":"stream","text":["SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\n","syswgetrc = C:\\Program Files (x86)\\GnuWin32/etc/wgetrc\n","--2022-11-23 15:55:08--  http://cvgl.stanford.edu/data2/ShapeNetVox32.tgz\n","Resolving cvgl.stanford.edu... 171.64.64.64\n","Connecting to cvgl.stanford.edu|171.64.64.64|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://cvgl.stanford.edu/data2/ShapeNetVox32.tgz [following]\n","--2022-11-23 15:55:08--  https://cvgl.stanford.edu/data2/ShapeNetVox32.tgz\n","Connecting to cvgl.stanford.edu|171.64.64.64|:443... connected.\n","Unable to establish SSL connection.\n","tar: Error opening archive: Failed to open 'exercise_2/data/ShapeNetVox32.tgz'\n"]},{"name":"stdout","output_type":"stream","text":["Done.\n"]},{"name":"stderr","output_type":"stream","text":["'rm' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["print('Downloading ...')\n","!wget http://cvgl.stanford.edu/data2/ShapeNetVox32.tgz -P exercise_2/data\n","print('Extracting ...')\n","!tar -xzf exercise_2/data/ShapeNetVox32.tgz -C exercise_2/data\n","!rm exercise_2/data/ShapeNetVox32.tgz\n","print('Done.')"]},{"cell_type":"markdown","metadata":{"id":"bEWiFRMUdsGW","pycharm":{"name":"#%% md\n"}},"source":["### (b) Dataloading and exploring the data\n","\n","We already provide you with a training and validation split in files `train.txt` and `val.txt` in folder `exercise_2/data/splits/shapenet`.\n","All the shapes in the list `train.txt` make up the training samples, while all the samples in `val.txt` constitute the validation set.\n","Additionally, we provide `overfit.txt` as the set of shapes we'll use for overfitting / debugging later.\n","\n","Now let's write a Pytorch Dataset class that can load this data from the disk. Check out `ShapeNetVox` class in file `exercise_2/data/shapenet.py`\n","for a partial implementation of such a dataset.\n","\n","The dataset class is instantiated with the type of the split, e.g. `train`, `val` or `overfit` and loads all\n","shape names for that split as a list in its member variable `self.items`. The class also provides utility method `get_shape_voxels(shapenet_id)`\n","which given a `shapenet_id` of the form `<shape_class>/<shape_identifier>` returns a 32x32x32 numpy array representing the voxels of the shape.\n","The class also provides a list of all shape categories as the static member `ShapeNetVox.classes`.\n","\n","Your task is to fill out the missing implementations of functions `__getitem__` and `__len__` as specified by their docstrings.\n","Once done, test your implementation below."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669154926348,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"oXgBN4XhdsGX","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from exercise_2.data.shapenet import ShapeNetVox\n","# Let's test your implementation"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1669154924420,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"KeU0ipWodsGX","outputId":"c931bda4-b35b-44e1-f4ff-73fff5f6ea96","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of train set: 21705\n"]}],"source":["# Create a dataset with train split\n","trainset = ShapeNetVox('train')\n","\n","# Get length, which is a call to __len__ function\n","print(f'Length of train set: {len(trainset)}')  # expected output: 21705"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669154928399,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"V_a9IxADdsGY","outputId":"0a8d7832-27a3-4fcb-9a5d-91302222aa9b","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of validation set: 5426\n"]}],"source":["# Create a dataset with val split and print its length\n","valset = ShapeNetVox('val')\n","print(f'Length of validation set: {len(valset)}')  # expected output: 5426"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":601,"referenced_widgets":["a9e7eafeae5e4f17a28407e2e383f6a4","63217bb57c5344daa340ebd060fc095a","910e9b0d4ad145d48f20e220c2f30041","dfeae9c358d646c38ff0c6cbe7e50d25"]},"executionInfo":{"elapsed":779,"status":"ok","timestamp":1669154869000,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"xoCFwHlEdsGZ","outputId":"f3b7ad51-4df5-4347-ad78-5b6a40374df2","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: 04379243/d120d47f8c9bc5028640bc5712201c4a\n","Voxel Dimensions: (1, 32, 32, 32)\n","Label: 10 | 04379243\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"158abdd0347f44a9b1b9184f35ceca1f","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Visualize some shapes\n","from exercise_2.util.visualization import visualize_occupancy\n","\n","shape_data = trainset[0]\n","print(f'Name: {shape_data[\"name\"]}')  # expected output: 04379243/d120d47f8c9bc5028640bc5712201c4a\n","print(f'Voxel Dimensions: {shape_data[\"voxel\"].shape}')  # expected output: (1, 32, 32, 32)\n","print(f'Label: {shape_data[\"label\"]} | {ShapeNetVox.classes[shape_data[\"label\"]]}')  # expected output: 10, 04379243\n","\n","visualize_occupancy(shape_data[\"voxel\"].squeeze(), flip_axes=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":601,"referenced_widgets":["70c669c566ef46798964991bb8427b64","66f954ed799e470692b17c9f7bc345b3","27f66f0921e443f1a2bbf1a61db52c67","980fb31a97f34092b57c0d9e76e1ecf8"]},"executionInfo":{"elapsed":696,"status":"ok","timestamp":1669154936507,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"LJ0CMo9OdsGZ","outputId":"14856e86-70ee-4846-f75e-071b2e8f5515","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: 03211117/edfc3a8ecc5b07e9feb0fb1dff94c98a\n","Voxel Dimensions: (1, 32, 32, 32)\n","Label: 5 | 03211117\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fc106ea7bb448f3941fbad57ea7b335","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["shape_data = trainset[7]\n","print(f'Name: {shape_data[\"name\"]}')  # expected output: 03211117/edfc3a8ecc5b07e9feb0fb1dff94c98a\n","print(f'Voxel Dimensions: {shape_data[\"voxel\"].shape}')  # expected output: (1, 32, 32, 32)\n","print(f'Label: {shape_data[\"label\"]} | {ShapeNetVox.classes[shape_data[\"label\"]]}')  # expected output: 5, 03211117\n","\n","visualize_occupancy(shape_data[\"voxel\"].squeeze(), flip_axes=True)"]},{"cell_type":"markdown","metadata":{"id":"XM41UsgzdsGa","pycharm":{"name":"#%% md\n"}},"source":["### (c) Defining the model\n","\n","We'll be using the 3DCNN model introduced by Qi et al.[1] to classify shapes. In particular, we'll be using\n","the \"Auxiliary Training by Subvolume Supervision\" model presented in `Section 4.2` / `Fig. 3` of the paper.\n","\n","<img src=\"exercise_2/images/3dcnn.png\" alt=\"3dcnn_architecture\" style=\"width: 1024px;\"/>\n","\n","Fill in the model implementation of `MLPConv` and `ThreeDeeCNN`(which would use `MLPConv` layers) in the file `exercise_2/model/cnn3d.py`.\n","Since the 3DCNN model predicts 9 labels for each samples (1 global + 8 local), we expect the output tensor to be of the shape `(B, 9, N_cls)` where `B` is the batch size, `N_cls` are number of classes (13 in our case).\n","This means that the global and auxilary parts of the network output a score per class.\n","\n","Here are some basic sanity tests for your implemetation."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1669154955681,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"HSbEXcLIdsGb","outputId":"e45c57ac-d653-4c2f-94ef-f23d96fb6e42","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["   | Name                 | Type        | Params  \n","--------------------------------------------------------\n","0  | backbone             | Sequential  | 3764464 \n","1  | backbone.0           | MLPConv     | 15120   \n","2  | backbone.0.model     | Sequential  | 15120   \n","3  | backbone.0.model.0   | Conv3d      | 10416   \n","4  | backbone.0.model.1   | ReLU        | 0       \n","5  | backbone.0.model.2   | Conv3d      | 2352    \n","6  | backbone.0.model.3   | ReLU        | 0       \n","7  | backbone.0.model.4   | Conv3d      | 2352    \n","8  | backbone.0.model.5   | ReLU        | 0       \n","9  | backbone.1           | MLPConv     | 1011680 \n","10 | backbone.1.model     | Sequential  | 1011680 \n","11 | backbone.1.model.0   | Conv3d      | 960160  \n","12 | backbone.1.model.1   | ReLU        | 0       \n","13 | backbone.1.model.2   | Conv3d      | 25760   \n","14 | backbone.1.model.3   | ReLU        | 0       \n","15 | backbone.1.model.4   | Conv3d      | 25760   \n","16 | backbone.1.model.5   | ReLU        | 0       \n","17 | backbone.2           | MLPConv     | 2737664 \n","18 | backbone.2.model     | Sequential  | 2737664 \n","19 | backbone.2.model.0   | Conv3d      | 2212352 \n","20 | backbone.2.model.1   | ReLU        | 0       \n","21 | backbone.2.model.2   | Conv3d      | 262656  \n","22 | backbone.2.model.3   | ReLU        | 0       \n","23 | backbone.2.model.4   | Conv3d      | 262656  \n","24 | backbone.2.model.5   | ReLU        | 0       \n","25 | partial_predictors   | ModuleList  | 53352   \n","26 | partial_predictors.0 | Linear      | 6669    \n","27 | partial_predictors.1 | Linear      | 6669    \n","28 | partial_predictors.2 | Linear      | 6669    \n","29 | partial_predictors.3 | Linear      | 6669    \n","30 | partial_predictors.4 | Linear      | 6669    \n","31 | partial_predictors.5 | Linear      | 6669    \n","32 | partial_predictors.6 | Linear      | 6669    \n","33 | partial_predictors.7 | Linear      | 6669    \n","34 | full_predictor       | Sequential  | 12613645\n","35 | full_predictor.0     | Linear      | 8390656 \n","36 | full_predictor.1     | ReLU        | 0       \n","37 | full_predictor.2     | Linear      | 4196352 \n","38 | full_predictor.3     | ReLU        | 0       \n","39 | full_predictor.4     | Linear      | 26637   \n","40 | TOTAL                | ThreeDeeCNN | 16431461\n","Output tensor shape:  torch.Size([8, 9, 13])\n","Number of traininable params: 16.43M\n"]}],"source":["from exercise_2.model.cnn3d import ThreeDeeCNN\n","from exercise_2.util.model import summarize_model\n","\n","cnn3d = ThreeDeeCNN(13)\n","print(summarize_model(cnn3d))  # Expected: Rows 0-40 and TOTAL = 16431461\n","\n","input_tensor = torch.randn(8, 1, 32, 32, 32)\n","predictions = cnn3d(input_tensor)\n","\n","print('Output tensor shape: ', predictions.shape)  # expected output: 8, 9, 13\n","\n","num_trainable_params = sum(p.numel() for p in cnn3d.parameters() if p.requires_grad) / 1e6\n","print(f'Number of traininable params: {num_trainable_params:.2f}M')  # expected output: ~16M"]},{"cell_type":"markdown","metadata":{"id":"JiYsCJPudsGc"},"source":["### (d) Training Script\n","\n","Now that we have the dataset class and the model class, we just need a training script\n","that trains the model using data from train dataset and evaluates the model's validation\n","performance during training. A partial script is provided in `exercise_2/training/train_3dcnn.py`.\n","Fill in the missing blocks to make the training work."]},{"cell_type":"markdown","metadata":{"id":"5fZRmUsZdsGc"},"source":["### (e) Overfitting to a few samples\n","Before training on entire data, it is usually a good idea to try training on a small subset of data,\n","to see if your model can do forward and backward passes without any errors, your metrics work, and that you model can\n","overfit with a very low error on this small set."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94771,"status":"ok","timestamp":1669154716672,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"U7l7LszddsGd","outputId":"2cb14eac-77bd-4a69-d2dc-d84b12304fa2","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CPU\n","[024/00003] train_loss: 20.582\n","[024/00003] val_loss: 16.292, val_accuracy: 31.250%\n","[049/00003] train_loss: 7.111\n","[049/00003] val_loss: 0.805, val_accuracy: 93.750%\n","[074/00003] train_loss: 1.326\n","[074/00003] val_loss: 0.049, val_accuracy: 100.000%\n","[099/00003] train_loss: 0.041\n","[099/00003] val_loss: 0.008, val_accuracy: 100.000%\n","[124/00003] train_loss: 0.004\n","[124/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[149/00003] train_loss: 0.000\n","[149/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[174/00003] train_loss: 0.000\n","[174/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[199/00003] train_loss: 0.000\n","[199/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[224/00003] train_loss: 0.000\n","[224/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[249/00003] train_loss: 0.000\n","[249/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[274/00003] train_loss: 0.000\n","[274/00003] val_loss: 0.000, val_accuracy: 100.000%\n","[299/00003] train_loss: 0.000\n","[299/00003] val_loss: 0.000, val_accuracy: 100.000%\n"]}],"source":["from exercise_2.training import train_3dcnn\n","config = {\n","    'experiment_name': '2_3_3dcnn_overfitting',\n","    'device': 'cpu',                      # change this to cpu if you do not have a GPU\n","    'is_overfit': True,                      # True since we're doing overfitting\n","    'batch_size': 16,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.0005,\n","    'max_epochs': 300,\n","    'print_every_n': 100,\n","    'validate_every_n': 100,\n","}\n","\n","train_3dcnn.main(config)  # should be able to get ~0 loss, 100% accuracy"]},{"cell_type":"markdown","metadata":{"id":"flnUF_avdsGe","pycharm":{"name":"#%% md\n"}},"source":["### (f) Training over the entire training set\n","If the overfitting works, we can go ahead with training on the entire dataset."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298302,"status":"ok","timestamp":1669155261715,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"y_zLiHk4dsGe","outputId":"e8fcf058-b76b-4f2f-9cae-38b71a94c4af","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CPU\n","[000/00099] train_loss: 21.146\n","[000/00199] train_loss: 15.431\n","[000/00249] val_loss: 9.080, val_accuracy: 68.750%\n","[000/00299] train_loss: 10.090\n","[000/00399] train_loss: 7.703\n","[000/00499] train_loss: 5.704\n","[000/00499] val_loss: 8.391, val_accuracy: 68.750%\n","[000/00599] train_loss: 5.274\n","[000/00699] train_loss: 4.782\n","[000/00749] val_loss: 1.271, val_accuracy: 100.000%\n","[000/00799] train_loss: 4.612\n","[000/00899] train_loss: 4.609\n","[000/00999] train_loss: 4.527\n","[000/00999] val_loss: 4.148, val_accuracy: 87.500%\n","[000/01099] train_loss: 4.117\n","[000/01199] train_loss: 3.881\n","[000/01249] val_loss: 0.994, val_accuracy: 100.000%\n","[000/01299] train_loss: 4.177\n","[001/00042] train_loss: 3.988\n","[001/00142] train_loss: 3.496\n","[001/00142] val_loss: 1.536, val_accuracy: 100.000%\n","[001/00242] train_loss: 3.747\n","[001/00342] train_loss: 3.770\n","[001/00392] val_loss: 3.565, val_accuracy: 93.750%\n","[001/00442] train_loss: 3.807\n","[001/00542] train_loss: 3.407\n","[001/00642] train_loss: 3.310\n","[001/00642] val_loss: 1.809, val_accuracy: 100.000%\n","[001/00742] train_loss: 3.310\n","[001/00842] train_loss: 3.033\n","[001/00892] val_loss: 2.671, val_accuracy: 100.000%\n","[001/00942] train_loss: 3.225\n","[001/01042] train_loss: 3.264\n","[001/01142] train_loss: 3.159\n","[001/01142] val_loss: 0.601, val_accuracy: 100.000%\n","[001/01242] train_loss: 3.007\n","[001/01342] train_loss: 2.860\n","[002/00035] val_loss: 2.970, val_accuracy: 87.500%\n","[002/00085] train_loss: 3.232\n","[002/00185] train_loss: 2.938\n","[002/00285] train_loss: 3.068\n","[002/00285] val_loss: 4.182, val_accuracy: 87.500%\n","[002/00385] train_loss: 2.841\n","[002/00485] train_loss: 2.895\n","[002/00535] val_loss: 0.500, val_accuracy: 100.000%\n","[002/00585] train_loss: 2.908\n","[002/00685] train_loss: 3.024\n","[002/00785] train_loss: 2.694\n","[002/00785] val_loss: 1.442, val_accuracy: 100.000%\n","[002/00885] train_loss: 2.520\n","[002/00985] train_loss: 2.883\n","[002/01035] val_loss: 1.883, val_accuracy: 87.500%\n","[002/01085] train_loss: 3.160\n","[002/01185] train_loss: 2.640\n","[002/01285] train_loss: 2.484\n","[002/01285] val_loss: 2.012, val_accuracy: 100.000%\n","[003/00028] train_loss: 2.716\n","[003/00128] train_loss: 2.307\n","[003/00178] val_loss: 0.806, val_accuracy: 100.000%\n","[003/00228] train_loss: 2.199\n","[003/00328] train_loss: 2.536\n","[003/00428] train_loss: 2.483\n","[003/00428] val_loss: 1.626, val_accuracy: 93.750%\n","[003/00528] train_loss: 2.703\n","[003/00628] train_loss: 2.575\n","[003/00678] val_loss: 2.443, val_accuracy: 93.750%\n","[003/00728] train_loss: 2.506\n","[003/00828] train_loss: 2.576\n","[003/00928] train_loss: 2.449\n","[003/00928] val_loss: 4.252, val_accuracy: 93.750%\n","[003/01028] train_loss: 2.720\n","[003/01128] train_loss: 2.579\n","[003/01178] val_loss: 1.076, val_accuracy: 100.000%\n","[003/01228] train_loss: 2.621\n","[003/01328] train_loss: 2.453\n","[004/00071] train_loss: 2.287\n","[004/00071] val_loss: 3.386, val_accuracy: 93.750%\n","[004/00171] train_loss: 2.502\n","[004/00271] train_loss: 2.352\n","[004/00321] val_loss: 1.414, val_accuracy: 93.750%\n","[004/00371] train_loss: 2.123\n","[004/00471] train_loss: 2.164\n","[004/00571] train_loss: 2.226\n","[004/00571] val_loss: 2.739, val_accuracy: 93.750%\n","[004/00671] train_loss: 2.092\n","[004/00771] train_loss: 2.403\n","[004/00821] val_loss: 1.033, val_accuracy: 100.000%\n","[004/00871] train_loss: 2.324\n","[004/00971] train_loss: 2.362\n","[004/01071] train_loss: 2.334\n","[004/01071] val_loss: 1.896, val_accuracy: 93.750%\n","[004/01171] train_loss: 2.084\n","[004/01271] train_loss: 2.254\n","[004/01321] val_loss: 2.371, val_accuracy: 93.750%\n"]}],"source":["config = {\n","    'experiment_name': '2_3_3dcnn_generalization',\n","    'device': 'cpu',                     # change this to cpu if you do not have a GPU\n","    'is_overfit': False,\n","    'batch_size': 16,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.0005,\n","    'max_epochs': 5,\n","    'print_every_n': 100,\n","    'validate_every_n': 250,\n","}\n","\n","train_3dcnn.main(config)                    # should have accuracy > 88% on val set"]},{"cell_type":"markdown","metadata":{"id":"sW15DcmLdsGf"},"source":["### (g) Inference using the trained model\n","\n","We would now like to make shape category inference given shapes from validation set.\n","Implement the function `infer_single` of class `InferenceHandler3DCNN` in file `exercise_2/inference/infer_3dcnn.py` such that it returns the\n","shape category name predicted by the model given its numpy voxel representation (32x32x32). Note that the network predicts a label in range [0, 12],\n","which can be mapped to a class ID (e.g. 03001627) using `ShapeNetVox.classes` which can further be mapped to a category name (e.g. chair for 03001627) using\n","`ShapeNetVox.class_name_mapping`."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1669156734991,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"OXA8QMGFdsGg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from exercise_2.inference.infer_3dcnn import InferenceHandler3DCNN\n","\n","# create a handler for inference using a trained checkpoint\n","inferer = InferenceHandler3DCNN('exercise_2/runs/2_3_3dcnn_generalization/model_best.ckpt')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529,"referenced_widgets":["f154013bd98144ea8641c8430215cb1a","02db40411a4c40c8a1ffa4ace12b60ab","42b279636ea04f39b740dac04d4eb495","d5ec128b778a4902b06e134fbb526d4e"]},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1669156737602,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"FhPebYsMdsGg","outputId":"44f16968-80f9-4e63-a831-ff5c03c13073","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7f1392493c64b9d80d338408ed6cf6d","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# get shape voxels and visualize\n","shape_voxels = ShapeNetVox.get_shape_voxels('03001627/f913501826c588e89753496ba23f2183')\n","visualize_occupancy(shape_voxels, flip_axes=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529,"status":"ok","timestamp":1669156578441,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"AFo5Z_dcdsGh","outputId":"535712e6-8d9b-4715-c1d2-2fd4433cc7fd","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted category: chair\n"]}],"source":["# predict category\n","print('Predicted category:', inferer.infer_single(shape_voxels))  # expected output: chair"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529,"referenced_widgets":["8719f752eb3e4ef3b9030dfb16b4a7c5","ad1e685c52e94d66879a519023fb66ee","21d33b16e37a4876b56e6066cad89915","1bcf1bdd64cc46988b06dee548a87c9c"]},"executionInfo":{"elapsed":1049,"status":"ok","timestamp":1669156754967,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"7GDK-bJWdsGi","outputId":"9618e771-c848-4710-9edb-9d37757ecde0","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5ba447a76b440338b4f63518bb3af29","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# get shape voxels and visualize\n","shape_voxels = ShapeNetVox.get_shape_voxels('02691156/6af4383123972f2262b600da24e0965')\n","visualize_occupancy(shape_voxels, flip_axes=True)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1669156758657,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"8kXC95mUdsGi","outputId":"dc07da7c-4f92-4d61-e7f6-8cdffe49eb92","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted category: airplane\n"]}],"source":["# predict category\n","print('Predicted category:', inferer.infer_single(shape_voxels))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529,"referenced_widgets":["38e52805a06741f8b105ed14c51e965b","fed0d65396834ecb8a252edc067f469f","1eece8487e41493289f1f8c6bd921431","eef1fb5f4a5c4b5fa418b04e0d031f36"]},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1669156761825,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"QYGVLMu4dsGj","outputId":"63688f4f-0046-42f3-89a7-b53f2b2f93b7","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a97a6de26f9452da7fbf0d49ae4c82c","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# get shape voxels and visualize\n","shape_voxels = ShapeNetVox.get_shape_voxels('04090263/eae96ddf483e896c805d3d8e378d155e')\n","visualize_occupancy(shape_voxels, flip_axes=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":470,"status":"ok","timestamp":1669156769457,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"Xl1T5xx1dsGj","outputId":"cd349414-8cb3-4250-cd2a-b618914028af","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted category: rifle\n"]}],"source":["# predict category\n","print('Predicted category:', inferer.infer_single(shape_voxels))"]},{"cell_type":"markdown","metadata":{"id":"EOMw8iGfdsGk"},"source":["Make sure you submit the trained model `exercise_2/runs/2_3_3dcnn_generalization/model_best.ckpt` in your zip\n","so that we can evaluate it on the test set at our end."]},{"cell_type":"markdown","metadata":{"id":"ik_MM45kdsGk","pycharm":{"name":"#%% md\n"}},"source":["## 2.4. Shape Classification using PointNet"]},{"cell_type":"markdown","metadata":{"id":"AAz6gl8RdsGk"},"source":["The approach you used above works very well when there is voxelized data available. However, many applications require that points are used instead and voxelizing them every time is cumbersome and might lead to unexpected artifacts.\n","\n","This is where PointNet [2] is very useful: It directly takes in a set of points and can perform both classification and semantic segmentation without the need for any gridification or other form of conversion.\n","\n","You already heard a lot about the approach and architecture in the lecture; here, our goal is to create an implementation of it from scratch and try it out on some ShapeNet data."]},{"cell_type":"markdown","metadata":{"id":"lbkgBo1-dsGl"},"source":["### (a) Download and prepare the ShapeNetPointClouds dataset\n","We generated point clouds from ShapeNet meshes via uniform sampling. Each point cloud contains 1024 xyz points.\n","\n","The data layout is basically the same as in 2.3.:\n","Each folder in the `exercise_2/data/ShapeNetPointClouds` directory contains one shape category represented by a number, e.g. `02691156`.\n","We provide the mapping between these numbers and the corresponding names in `exercise_2/data/shape_info.json`. Each of these shape category folders contains a number of shapes in obj format.\n","\n","```\n","# contents of exercise_2/data/ShapeNetPointClouds\n","\n","02691156/                                      # Shape category folder with all its shapes\n","    ├── 1a04e3eab45ca15dd86060f189eb133.obj    # A single shape of the category\n","    ├── 1a6ad7a24bb89733f412783097373bdc.obj   # Another shape of the category\n","    ├── :                                      # And so on ...\n","    ├── :\n","02828884/                                      # Another shape category folder\n","02933112/                                      # In total you should have 13 shape category folders\n",":\n",":\n","```"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104420,"status":"ok","timestamp":1669156357122,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"2TkjYCDRdsGm","outputId":"adbd00a0-5362-4aac-d344-b7dbeec942aa","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading ...\n","--2022-11-22 22:30:52--  http://kaldir.vc.in.tum.de/cdiller/ShapeNetPointClouds.zip\n","Resolving kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)... 131.159.40.125\n","Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.40.125|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://kaldir.vc.in.tum.de:443/cdiller/ShapeNetPointClouds.zip [following]\n","--2022-11-22 22:30:52--  https://kaldir.vc.in.tum.de/cdiller/ShapeNetPointClouds.zip\n","Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.40.125|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 453765424 (433M) [application/zip]\n","Saving to: ‘exercise_2/data/ShapeNetPointClouds.zip’\n","\n","ShapeNetPointClouds 100%[===================>] 432.74M  5.25MB/s    in 89s     \n","\n","2022-11-22 22:32:22 (4.88 MB/s) - ‘exercise_2/data/ShapeNetPointClouds.zip’ saved [453765424/453765424]\n","\n","Extracting ...\n","Done.\n"]}],"source":["print('Downloading ...')\n","!wget http://kaldir.vc.in.tum.de/cdiller/ShapeNetPointClouds.zip -P exercise_2/data\n","print('Extracting ...')\n","!unzip -q exercise_2/data/ShapeNetPointClouds.zip -d exercise_2/data\n","!rm exercise_2/data/ShapeNetPointClouds.zip\n","print('Done.')"]},{"cell_type":"markdown","metadata":{"id":"FywurCTcdsGm"},"source":["### (b) Dataset implementation\n","\n","You can use the same split setup as in 2.3: `overfit.txt` for overfitting, `train.txt` for the train samples, and `val.txt` for the val samples in folder `exercise_2/data/splits/shapenet`.\n","\n","The dataset implementation will therefore be very similar to the one from 2.3: Fill out the missing implementations of functions `__getitem__` and `__len__` in class `ShapeNetPoints` in `exercise_2/data/shapenet.py`.\n","\n","The major difference is how the actual data is loaded: We don't have regular voxel grids anymore and instead load arrays of 1024 points each. In `__getitem__`, we now return 'points' instead of 'voxel' and for loading the point clouds, we use `get_point_cloud` instead of `get_shape_voxels`. You can load the point cloud data either by hand (since it is in the same obj format you used in exercise 1) or simply use `trimesh.load`. The point clouds you return from `__getitem__` should have shape 3 x 1024 and datatype `np.float32`.\n","\n","Otherwise, the implementation is very much the same as in 2.3. Once done, test your implementation below."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":645,"status":"ok","timestamp":1669159719227,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"zahzp3QSdsGn","outputId":"9788113f-0e4b-464b-a7e6-ed7b771d19b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of train set: 21705\n","Length of val set: 5426\n","Length of overfit set: 64\n"]}],"source":["from exercise_2.data.shapenet import ShapeNetPoints\n","\n","# Create a dataset with train split\n","train_dataset = ShapeNetPoints('train')\n","val_dataset = ShapeNetPoints('val')\n","overfit_dataset = ShapeNetPoints('overfit')\n","\n","# Get length, which is a call to __len__ function\n","print(f'Length of train set: {len(train_dataset)}')  # expected output: 21705\n","# Get length, which is a call to __len__ function\n","print(f'Length of val set: {len(val_dataset)}')  # expected output: 5426\n","# Get length, which is a call to __len__ function\n","print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581,"referenced_widgets":["31bd18878d494ec380d910b75392cb8f","4b2f6fd1a85842afb5030a511fe8cbcc","cee74d91cf134277b7341cf7e27d28a4","ce7a9238aa5c47d3ba7b0a4533c810b6"]},"executionInfo":{"elapsed":546,"status":"ok","timestamp":1669159950612,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"U_wgo5E-dsGo","outputId":"068015b9-231e-4680-fd8e-3ad37e639fd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: 03636649/5f3f11372141da8def0b2fc3511b6fbd\n","Voxel Dimensions: (3, 1024)\n","Label: 6 | 03636649 | lamp\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e41066f87dc54047bb9795d42f07cc0d","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Visualize some shapes\n","from exercise_2.util.visualization import visualize_pointcloud\n","\n","shape_data = train_dataset[np.random.randint(len(train_dataset))]\n","print(f'Name: {shape_data[\"name\"]}')  # expected output: 04379243/d120d47f8c9bc5028640bc5712201c4a\n","print(f'Voxel Dimensions: {shape_data[\"points\"].shape}')  # expected output: (3, 1024)\n","print(f'Label: {shape_data[\"label\"]} | {ShapeNetPoints.classes[shape_data[\"label\"]]} | {ShapeNetPoints.class_name_mapping[ShapeNetPoints.classes[shape_data[\"label\"]]]}')  # expected output: 10, 04379243\n","\n","visualize_pointcloud(shape_data[\"points\"].T, point_size=0.025, flip_axes=True)"]},{"cell_type":"markdown","metadata":{"id":"wDGW9DBedsGo"},"source":["### (c) Defining the model\n","\n","The model architecture of PointNet was discussed in the lecture and is visualized below:\n","<img src=\"exercise_2/images/pointnet.png\" alt=\"pointnet_architecture\" style=\"width: 800px;\"/>\n","\n","Some hints for the actual implementation:\n","1. We use conv1d layers with kernel size 1 for all \"shared\" mlps to expand the feature channel dimension, e.g. when the input is of shape batch_size x 3 x 1024 and we apply conv1d(in_features=3, out_features=64), then we get to shape batch_size x 64 x 1024\n","2. The mlps in the classification network after the max pooling operation are implemented using Linear layers\n","3. The numbers in parenthesis after mlp() in the visualization above descibe the number of layers with their out_features dimension. Note though that the first mlp from nx3 to nx64 is expressed as two layers in the original tensorflow code but can be implemented as a single conv1d layer going from 3 to 64 features in the pytorch version.\n","4. We define all layers up to and including the max pooling operation as the `PointNetEncoder`. The architecture of the model head depends on the task we are trying to solve: Either classification (`PointNetClassification`, used in this part of the exercise) or segmentation (`PointNetSegmentation`, used in 2.5).\n","5. ReLU and Batch Norms are applied after each layer, except after the last classification layer. In the last layer before the max operation, we only apply Batch Norm but no ReLU.\n","6. Dropout is applied for classification only after the second Linear layer, before the Batch Norm.\n","7. The TNets are basically small PointNets.\n","\n","Implement the missing parts of the PointNet architecture in `TNet`, `PointNetEncoder`, and `PointNetClassification`, as indicated by the TODOs. All of them are located in `exercise_2/models/pointnet.py`. Use the following code cell to sanity check your implementation:"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1490,"status":"ok","timestamp":1669162074894,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"WIUbgGGIdsGp","outputId":"30757035-e2b1-43a8-9887-ecdf2c16add6"},"outputs":[{"name":"stdout","output_type":"stream","text":["   | Name                                     | Type                   | Params \n","--------------------------------------------------------------------------------------\n","0  | encoder                                  | PointNetEncoder        | 2803529\n","1  | encoder.block_1                          | Sequential             | 384    \n","2  | encoder.block_1.0                        | Conv1d                 | 256    \n","3  | encoder.block_1.1                        | BatchNorm1d            | 128    \n","4  | encoder.block_1.2                        | ReLU                   | 0      \n","5  | encoder.block_2                          | Sequential             | 142720 \n","6  | encoder.block_2.0                        | Conv1d                 | 8320   \n","7  | encoder.block_2.1                        | BatchNorm1d            | 256    \n","8  | encoder.block_2.2                        | ReLU                   | 0      \n","9  | encoder.block_2.3                        | Conv1d                 | 132096 \n","10 | encoder.block_2.4                        | BatchNorm1d            | 2048   \n","11 | encoder.input_transform_net              | TNet                   | 803081 \n","12 | encoder.input_transform_net.backbone     | Sequential             | 143104 \n","13 | encoder.input_transform_net.backbone.0   | Conv1d                 | 256    \n","14 | encoder.input_transform_net.backbone.1   | BatchNorm1d            | 128    \n","15 | encoder.input_transform_net.backbone.2   | ReLU                   | 0      \n","16 | encoder.input_transform_net.backbone.3   | Conv1d                 | 8320   \n","17 | encoder.input_transform_net.backbone.4   | BatchNorm1d            | 256    \n","18 | encoder.input_transform_net.backbone.5   | ReLU                   | 0      \n","19 | encoder.input_transform_net.backbone.6   | Conv1d                 | 132096 \n","20 | encoder.input_transform_net.backbone.7   | BatchNorm1d            | 2048   \n","21 | encoder.input_transform_net.backbone.8   | ReLU                   | 0      \n","22 | encoder.input_transform_net.fcl          | Sequential             | 659977 \n","23 | encoder.input_transform_net.fcl.0        | Linear                 | 524800 \n","24 | encoder.input_transform_net.fcl.1        | BatchNorm1d            | 1024   \n","25 | encoder.input_transform_net.fcl.2        | Linear                 | 131328 \n","26 | encoder.input_transform_net.fcl.3        | BatchNorm1d            | 512    \n","27 | encoder.input_transform_net.fcl.4        | Linear                 | 2313   \n","28 | encoder.feature_transform_net            | TNet                   | 1857344\n","29 | encoder.feature_transform_net.backbone   | Sequential             | 147008 \n","30 | encoder.feature_transform_net.backbone.0 | Conv1d                 | 4160   \n","31 | encoder.feature_transform_net.backbone.1 | BatchNorm1d            | 128    \n","32 | encoder.feature_transform_net.backbone.2 | ReLU                   | 0      \n","33 | encoder.feature_transform_net.backbone.3 | Conv1d                 | 8320   \n","34 | encoder.feature_transform_net.backbone.4 | BatchNorm1d            | 256    \n","35 | encoder.feature_transform_net.backbone.5 | ReLU                   | 0      \n","36 | encoder.feature_transform_net.backbone.6 | Conv1d                 | 132096 \n","37 | encoder.feature_transform_net.backbone.7 | BatchNorm1d            | 2048   \n","38 | encoder.feature_transform_net.backbone.8 | ReLU                   | 0      \n","39 | encoder.feature_transform_net.fcl        | Sequential             | 1710336\n","40 | encoder.feature_transform_net.fcl.0      | Linear                 | 524800 \n","41 | encoder.feature_transform_net.fcl.1      | BatchNorm1d            | 1024   \n","42 | encoder.feature_transform_net.fcl.2      | Linear                 | 131328 \n","43 | encoder.feature_transform_net.fcl.3      | BatchNorm1d            | 512    \n","44 | encoder.feature_transform_net.fcl.4      | Linear                 | 1052672\n","45 | model                                    | Sequential             | 661005 \n","46 | model.0                                  | Linear                 | 524800 \n","47 | model.1                                  | BatchNorm1d            | 1024   \n","48 | model.2                                  | ReLU                   | 0      \n","49 | model.3                                  | Linear                 | 131328 \n","50 | model.4                                  | Dropout                | 0      \n","51 | model.5                                  | BatchNorm1d            | 512    \n","52 | model.6                                  | ReLU                   | 0      \n","53 | model.7                                  | Linear                 | 3341   \n","54 | TOTAL                                    | PointNetClassification | 3464534\n","Output tensor shape:  torch.Size([8, 13])\n","Number of traininable params: 3.46M\n"]}],"source":["from exercise_2.model.pointnet import PointNetClassification\n","from exercise_2.util.model import summarize_model\n","\n","pointnet = PointNetClassification(13)\n","print(summarize_model(pointnet))  # Expected: Rows 0-40 and TOTAL = 3464534\n","\n","input_tensor = torch.randn(8, 3, 1024)\n","predictions = pointnet(input_tensor)\n","\n","print('Output tensor shape: ', predictions.shape)  # Expected: 8, 13\n","num_trainable_params = sum(p.numel() for p in pointnet.parameters() if p.requires_grad) / 1e6\n","print(f'Number of traininable params: {num_trainable_params:.2f}M')  # Expected: ~3M"]},{"cell_type":"markdown","metadata":{"id":"fEONAuVadsGp"},"source":["### (d) Training Script and Overfitting\n","\n","You can now go to the train script in `train_pointnet_classification.py` and fill in the missing pieces as in 2.3. Then, verify that your training works by overfitting to a few samples below."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1669162133487,"user":{"displayName":"Mahmoud Ahmed","userId":"16222523276147953184"},"user_tz":-60},"id":"GJpuv3VNdsGq","outputId":"5326df06-c52d-4b31-d990-3443d66faf4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CPU\n","[049/00001] train_loss: 0.655\n","[049/00001] val_loss: 0.796, val_accuracy: 71.875%\n","[099/00001] train_loss: 0.189\n","[099/00001] val_loss: 0.229, val_accuracy: 96.875%\n","[149/00001] train_loss: 0.115\n","[149/00001] val_loss: 0.798, val_accuracy: 78.125%\n","[199/00001] train_loss: 0.068\n","[199/00001] val_loss: 0.370, val_accuracy: 93.750%\n","[249/00001] train_loss: 0.022\n","[249/00001] val_loss: 0.207, val_accuracy: 93.750%\n","[299/00001] train_loss: 0.040\n","[299/00001] val_loss: 0.256, val_accuracy: 90.625%\n"]}],"source":["from exercise_2.training import train_pointnet_classification\n","config = {\n","    'experiment_name': '2_4_pointnet_classification_overfitting',\n","    'device': 'cpu',                   # change this to cpu if you do not have a GPU\n","    'is_overfit': True,                   # True since we're doing overfitting\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.001,\n","    'max_epochs': 300,\n","    'print_every_n': 100,\n","    'validate_every_n': 100,\n","}\n","\n","train_pointnet_classification.main(config)  # should be able to get ~0 loss, 100% accuracy"]},{"cell_type":"markdown","metadata":{"id":"k5QN5pgMdsGq"},"source":["### (e) Training over the entire training set\n","\n","Once your overfitting completes successfully, you can move on to training on the entire dataset again."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yTJ4gHYOdsGq","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CPU\n","[000/00099] train_loss: 1.223\n","[000/00199] train_loss: 0.868\n","[000/00249] val_loss: 1.057, val_accuracy: 62.500%\n","[000/00299] train_loss: 0.620\n","[000/00399] train_loss: 0.611\n","[000/00499] train_loss: 0.570\n","[000/00499] val_loss: 0.658, val_accuracy: 81.250%\n","[000/00599] train_loss: 0.532\n","[001/00020] train_loss: 0.486\n","[001/00070] val_loss: 0.579, val_accuracy: 84.375%\n","[001/00120] train_loss: 0.457\n","[001/00220] train_loss: 0.511\n","[001/00320] train_loss: 0.454\n","[001/00320] val_loss: 0.495, val_accuracy: 81.250%\n","[001/00420] train_loss: 0.404\n","[001/00520] train_loss: 0.443\n","[001/00570] val_loss: 0.100, val_accuracy: 96.875%\n","[001/00620] train_loss: 0.440\n","[002/00041] train_loss: 0.413\n","[002/00141] train_loss: 0.380\n","[002/00141] val_loss: 0.171, val_accuracy: 93.750%\n","[002/00241] train_loss: 0.432\n","[002/00341] train_loss: 0.379\n","[002/00391] val_loss: 0.271, val_accuracy: 90.625%\n","[002/00441] train_loss: 0.369\n","[002/00541] train_loss: 0.357\n","[002/00641] train_loss: 0.408\n","[002/00641] val_loss: 0.368, val_accuracy: 87.500%\n","[003/00062] train_loss: 0.374\n","[003/00162] train_loss: 0.379\n","[003/00212] val_loss: 0.290, val_accuracy: 87.500%\n","[003/00262] train_loss: 0.364\n","[003/00362] train_loss: 0.350\n","[003/00462] train_loss: 0.363\n","[003/00462] val_loss: 0.512, val_accuracy: 84.375%\n","[003/00562] train_loss: 0.385\n","[003/00662] train_loss: 0.352\n","[004/00033] val_loss: 0.349, val_accuracy: 87.500%\n","[004/00083] train_loss: 0.378\n","[004/00183] train_loss: 0.337\n","[004/00283] train_loss: 0.340\n","[004/00283] val_loss: 0.360, val_accuracy: 78.125%\n","[004/00383] train_loss: 0.311\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [8], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexercise_2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train_pointnet_classification\n\u001b[0;32m      2\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m2_4_pointnet_classification_generalization\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m,                    \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvalidate_every_n\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m250\u001b[39m,\n\u001b[0;32m     12\u001b[0m }\n\u001b[1;32m---> 14\u001b[0m train_pointnet_classification\u001b[39m.\u001b[39;49mmain(config)  \u001b[39m# Should be able to get > 92% accuracy on the val set\u001b[39;00m\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\training\\train_pointnet_classification.py:155\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    151\u001b[0m Path(\n\u001b[0;32m    152\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexercise_2/runs/\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m\"\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m train(model, train_dataloader, val_dataloader, device, config)\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\training\\train_pointnet_classification.py:36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, valloader, device, config)\u001b[0m\n\u001b[0;32m     33\u001b[0m inputs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mpoints\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m prediction \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     38\u001b[0m loss_total \u001b[39m=\u001b[39m loss_criterion(prediction, labels)\n\u001b[0;32m     40\u001b[0m loss_total\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\model\\pointnet.py:125\u001b[0m, in \u001b[0;36mPointNetClassification.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 125\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    126\u001b[0m     \u001b[39m# TODO Pass output of encoder through your layers\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\model\\pointnet.py:87\u001b[0m, in \u001b[0;36mPointNetEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39m# TODO: First layer: 3->64\u001b[39;00m\n\u001b[0;32m     85\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_1(x)\n\u001b[1;32m---> 87\u001b[0m feature_transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_transform_net(x)\n\u001b[0;32m     88\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(x\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m), feature_transform)\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     89\u001b[0m point_features \u001b[39m=\u001b[39m x\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\Exercise_2\\exercise_2\\model\\pointnet.py:41\u001b[0m, in \u001b[0;36mTNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m b \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m \u001b[39m# TODO Pass input through layers, applying the same max operation as in PointNetEncoder\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# TODO No batch norm and relu after the last Linear layer\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[0;32m     42\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(x, \u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     43\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m)\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n","File \u001b[1;32md:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from exercise_2.training import train_pointnet_classification\n","config = {\n","    'experiment_name': '2_4_pointnet_classification_generalization',\n","    'device': 'cpu',                    # change this to cpu if you do not have a GPU\n","    'is_overfit': False,\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.001,\n","    'max_epochs': 10,\n","    'print_every_n': 100,\n","    'validate_every_n': 250,\n","}\n","\n","train_pointnet_classification.main(config)  # Should be able to get > 92% accuracy on the val set"]},{"cell_type":"markdown","metadata":{"id":"7ZNpnBnodsGr"},"source":["### (f) Inference using the trained model"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"H4-KU41FdsGr"},"outputs":[],"source":["from exercise_2.inference.infer_pointnet_classification import InferenceHandlerPointNetClassification\n","\n","\n","# create a handler for inference using a trained checkpoint\n","inferer = InferenceHandlerPointNetClassification('exercise_2/runs/2_4_pointnet_classification_generalization/model_best.ckpt')"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"9R6C8xQidsGs"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted category: chair\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d4f8e2da5944356b5c558b7b66682db","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# get shape point cloud and visualize\n","shape_points = ShapeNetPoints.get_point_cloud('03001627/f913501826c588e89753496ba23f2183')\n","print('Predicted category:', inferer.infer_single(shape_points))  # expected output: chair\n","visualize_pointcloud(shape_points.T, point_size=0.025, flip_axes=True)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ewiuGCKYdsGt"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted category: airplane\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69d70a26189e460a80c387adf4ebf1d9","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# get shape point cloud and visualize\n","shape_points = ShapeNetPoints.get_point_cloud('02691156/6af4383123972f2262b600da24e0965')\n","print('Predicted category:', inferer.infer_single(shape_points))\n","visualize_pointcloud(shape_points.T, point_size=0.025, flip_axes=True)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"R-yMtjXmdsGu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted category: rifle\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df652ce3c1ae469fb4da592ac5bd1414","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# get shape point cloud and visualize\n","shape_points = ShapeNetPoints.get_point_cloud('04090263/eae96ddf483e896c805d3d8e378d155e')\n","print('Predicted category:', inferer.infer_single(shape_points))\n","visualize_pointcloud(shape_points.T, point_size=0.025, flip_axes=True)"]},{"cell_type":"markdown","metadata":{"id":"2w8YAXMTdsGv"},"source":["Make sure you submit the trained model `exercise_2/runs/2_4_pointnet_classification_generalization/model_best.ckpt` in your zip\n","so that we can evaluate it on the test set at our end."]},{"cell_type":"markdown","metadata":{"id":"FDf5rZNxdsGv"},"source":["## 2.5. Shape Parts Segmentation using PointNet\n","\n","We now go one step further: We do not just want to learn the overall class label for a given shape but instead for each point in a shape the part it belongs to. We call this Part Segmentation. The good thing is that we can actually re-use most of the PointNet architecture from 2.4."]},{"cell_type":"markdown","metadata":{"id":"g3FO6moOdsGv"},"source":["### (a) Download the ShapeNetPart dataset\n","\n","Annotating data for segmentation is a lot of effort since labelling has to be performed within the shape for each part instead of globally for the entire shape.\n","\n","Luckily, there are existing datasets we can use for this. In our case, this is the ShapeNet Part Segmenation dataset that you can download in the cell below.\n","\n","In terms of data layout, the general idea of shape class identifiers and shape IDs is the same; we just have slightly different shape categories now. Also, each point cloud now has a correponding file specifying the part class for every point.\n","\n","We put the shape class labels for this dataset in `exercise_2/data/shape_parts_info.json`, analogous to `shape_info.json` from exercise parts 2.3 and 2.4.\n","\n","The point cloud data is stored as pts files which is basically an even simpler version of obj. It omits the v in front of each line that represents a point and does not support faces. Each line therefore represents one point with its xyz coordinates, separated by a space.\n","\n","```\n","# contents of exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0\n","\n","02691156/                                         # Shape category folder with all its shapes\n","    ├── points                                    # All point clouds go here\n","        ├── 1a04e3eab45ca15dd86060f189eb133.pts   # Point cloud data\n","        ├── 1a32f10b20170883663e90eaf6b4ca52.pts  # Another point cloud\n","        :\n","        :\n","    ├── points_label                              # Part labels for each point in the corresponding pts file\n","        ├── 1a04e3eab45ca15dd86060f189eb133.seg   # Each line represents the local part class of a point\n","        ├── 1a32f10b20170883663e90eaf6b4ca52.seg  # Another segmentation file\n","        :\n","        :\n","    ├── seg_img                                   # Visualizations of the original mesh part segmentation\n","02773838/                                         # Another shape category folder\n","02954340/                                         # In total you should have 16 shape category folders\n",":\n",":\n","train_test_split/                                 # Official split IDs\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dC5L6HhDdsGw"},"outputs":[],"source":["print('Downloading ...')\n","!wget https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate -P exercise_2/data\n","print('Extracting ...')\n","!unzip -q exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip -d exercise_2/data\n","!rm exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip\n","print('Done.')"]},{"cell_type":"markdown","metadata":{"id":"HjPfMvi8dsGx"},"source":["### (b) Dataset implementation\n","\n","You can use the same split setup as in 2.3 and 2.4: `overfit.txt` for overfitting, `train.txt` for the train samples, and `val.txt` for the val samples; This time, use the files in folder `exercise_2/data/splits/shapenet_parts`.\n","\n","The dataset implementation will be similar to 2.3 and 2.4: Fill out the missing implementations of functions `__getitem__` and `__len__` in class `ShapeNetPoints` in `exercise_2/data/shapenet_parts.py`. Note that you now need to load not only the point cloud but also the per-point segmentation labels in function `get_point_cloud_with_labels`. Since each point cloud in this dataset contains more than 1024 points, we also need to sub-sample the raw points list. Use `np.random.choice` for this: Randomizing the sampling will work as augmentation which in turn helps prevent overfitting. Make sure to sample the corresponding points and labels when doing so.\n","\n","Once done, test your implementation below."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Bu8i287mdsGx"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of train set: 12137\n","Length of val set: 1870\n","Length of overfit set: 64\n"]}],"source":["from exercise_2.data.shapenet_parts import ShapeNetParts\n","\n","# Create a dataset with train split\n","train_dataset = ShapeNetParts('train')\n","val_dataset = ShapeNetParts('val')\n","overfit_dataset = ShapeNetParts('overfit')\n","\n","# Get length, which is a call to __len__ function\n","print(f'Length of train set: {len(train_dataset)}')  # expected output: 12137\n","# Get length, which is a call to __len__ function\n","print(f'Length of val set: {len(val_dataset)}')  # expected output: 1870\n","# Get length, which is a call to __len__ function\n","print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"]},{"cell_type":"markdown","metadata":{"id":"gPz73wARdsGy"},"source":["### (c) Modifying the PointNet Model\n","\n","Take a look at the PointNet architecture again:\n","<img src=\"exercise_2/images/pointnet.png\" alt=\"pointnet_architecture\" style=\"width: 800px;\"/>\n","\n","We only cared about the blue classification part in 2.4. Now, we also want to implement the yellow part. You can re-use your encoder from 2.4. \n","\n","The idea is simple: Take the n points with 64-dimensional point features from the correct layer of the encoder and concatenate the global shape descriptor you get after applying the max function to it. Then, implement the remaining layers as conv1ds with batchnorm and relu after all but the last layer. The final layer reduces the dimensionality per point to m which is 50 in our case since we have 50 overall parts.\n","\n","Add the missing layers to `PointNetSegmentation` in `exercise_2/models/pointnet.py` and finish the implementation of the forward pass."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"-SuQ8fzKdsGz"},"outputs":[{"name":"stdout","output_type":"stream","text":["   | Name                                     | Type                 | Params \n","------------------------------------------------------------------------------------\n","0  | encoder                                  | PointNetEncoder      | 2803529\n","1  | encoder.block_1                          | Sequential           | 384    \n","2  | encoder.block_1.0                        | Conv1d               | 256    \n","3  | encoder.block_1.1                        | BatchNorm1d          | 128    \n","4  | encoder.block_1.2                        | ReLU                 | 0      \n","5  | encoder.block_2                          | Sequential           | 142720 \n","6  | encoder.block_2.0                        | Conv1d               | 8320   \n","7  | encoder.block_2.1                        | BatchNorm1d          | 256    \n","8  | encoder.block_2.2                        | ReLU                 | 0      \n","9  | encoder.block_2.3                        | Conv1d               | 132096 \n","10 | encoder.block_2.4                        | BatchNorm1d          | 2048   \n","11 | encoder.input_transform_net              | TNet                 | 803081 \n","12 | encoder.input_transform_net.backbone     | Sequential           | 143104 \n","13 | encoder.input_transform_net.backbone.0   | Conv1d               | 256    \n","14 | encoder.input_transform_net.backbone.1   | BatchNorm1d          | 128    \n","15 | encoder.input_transform_net.backbone.2   | ReLU                 | 0      \n","16 | encoder.input_transform_net.backbone.3   | Conv1d               | 8320   \n","17 | encoder.input_transform_net.backbone.4   | BatchNorm1d          | 256    \n","18 | encoder.input_transform_net.backbone.5   | ReLU                 | 0      \n","19 | encoder.input_transform_net.backbone.6   | Conv1d               | 132096 \n","20 | encoder.input_transform_net.backbone.7   | BatchNorm1d          | 2048   \n","21 | encoder.input_transform_net.backbone.8   | ReLU                 | 0      \n","22 | encoder.input_transform_net.fcl          | Sequential           | 659977 \n","23 | encoder.input_transform_net.fcl.0        | Linear               | 524800 \n","24 | encoder.input_transform_net.fcl.1        | BatchNorm1d          | 1024   \n","25 | encoder.input_transform_net.fcl.2        | Linear               | 131328 \n","26 | encoder.input_transform_net.fcl.3        | BatchNorm1d          | 512    \n","27 | encoder.input_transform_net.fcl.4        | Linear               | 2313   \n","28 | encoder.feature_transform_net            | TNet                 | 1857344\n","29 | encoder.feature_transform_net.backbone   | Sequential           | 147008 \n","30 | encoder.feature_transform_net.backbone.0 | Conv1d               | 4160   \n","31 | encoder.feature_transform_net.backbone.1 | BatchNorm1d          | 128    \n","32 | encoder.feature_transform_net.backbone.2 | ReLU                 | 0      \n","33 | encoder.feature_transform_net.backbone.3 | Conv1d               | 8320   \n","34 | encoder.feature_transform_net.backbone.4 | BatchNorm1d          | 256    \n","35 | encoder.feature_transform_net.backbone.5 | ReLU                 | 0      \n","36 | encoder.feature_transform_net.backbone.6 | Conv1d               | 132096 \n","37 | encoder.feature_transform_net.backbone.7 | BatchNorm1d          | 2048   \n","38 | encoder.feature_transform_net.backbone.8 | ReLU                 | 0      \n","39 | encoder.feature_transform_net.fcl        | Sequential           | 1710336\n","40 | encoder.feature_transform_net.fcl.0      | Linear               | 524800 \n","41 | encoder.feature_transform_net.fcl.1      | BatchNorm1d          | 1024   \n","42 | encoder.feature_transform_net.fcl.2      | Linear               | 131328 \n","43 | encoder.feature_transform_net.fcl.3      | BatchNorm1d          | 512    \n","44 | encoder.feature_transform_net.fcl.4      | Linear               | 1052672\n","45 | model                                    | Sequential           | 730034 \n","46 | model.0                                  | Conv1d               | 557568 \n","47 | model.1                                  | BatchNorm1d          | 1024   \n","48 | model.2                                  | ReLU                 | 0      \n","49 | model.3                                  | Conv1d               | 131328 \n","50 | model.4                                  | BatchNorm1d          | 512    \n","51 | model.5                                  | ReLU                 | 0      \n","52 | model.6                                  | Conv1d               | 32896  \n","53 | model.7                                  | BatchNorm1d          | 256    \n","54 | model.8                                  | ReLU                 | 0      \n","55 | model.9                                  | Conv1d               | 6450   \n","56 | TOTAL                                    | PointNetSegmentation | 3533563\n","Output tensor shape:  torch.Size([8, 1024, 50])\n","Number of traininable params: 3.53M\n"]}],"source":["from exercise_2.model.pointnet import PointNetSegmentation\n","from exercise_2.util.model import summarize_model\n","\n","pointnet = PointNetSegmentation(50)\n","print(summarize_model(pointnet))  # Expected: Rows 0-40 and TOTAL = 3533563\n","\n","input_tensor = torch.randn(8, 3, 1024)\n","predictions = pointnet(input_tensor)\n","\n","print('Output tensor shape: ', predictions.shape)  # Expected: 8, 1024, 50\n","num_trainable_params = sum(p.numel() for p in pointnet.parameters() if p.requires_grad) / 1e6\n","print(f'Number of traininable params: {num_trainable_params:.2f}M')  # Expected: ~3M"]},{"cell_type":"markdown","metadata":{"id":"28zdfoz6dsGz"},"source":["### (d) Training Script and Overfitting\n","\n","You can now go to the train script in `train_pointnet_segmentation.py` and fill in the missing pieces as in 2.3 and 2.4. Then, verify that your training work by overfitting to a few samples below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UPa2qx1dsG0"},"outputs":[],"source":["from exercise_2.training import train_pointnet_segmentation\n","config = {\n","    'experiment_name': '2_5_pointnet_segmentation_overfitting',\n","    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n","    'is_overfit': True,                   # True since we're doing overfitting\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.001,\n","    'max_epochs': 1000,\n","    'print_every_n': 100,\n","    'validate_every_n': 100,\n","}\n","\n","train_pointnet_segmentation.main(config)  # should be able to get ~0.03 loss, >97% accuracy, >0.95 iou"]},{"cell_type":"markdown","metadata":{"id":"c8iZ0c4DdsG0"},"source":["### (e) Training over the entire training set\n","\n","Once your overfitting completes successfully, you can move on to training on the entire dataset again."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"tm8Sa-5EdsG1","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n","[000/00099] train_loss: 1.289\n","[000/00199] train_loss: 0.604\n","[000/00249] val_loss: 0.448, val_accuracy: 84.027%, val_iou: 0.962\n","[000/00299] train_loss: 0.550\n","[001/00019] train_loss: 0.512\n","[001/00119] train_loss: 0.479\n","[001/00119] val_loss: 0.409, val_accuracy: 85.653%, val_iou: 0.966\n","[001/00219] train_loss: 0.474\n","[001/00319] train_loss: 0.463\n","[001/00369] val_loss: 0.378, val_accuracy: 86.374%, val_iou: 0.969\n","[002/00039] train_loss: 0.431\n","[002/00139] train_loss: 0.396\n","[002/00239] train_loss: 0.411\n","[002/00239] val_loss: 0.413, val_accuracy: 85.236%, val_iou: 0.969\n","[002/00339] train_loss: 0.404\n","[003/00059] train_loss: 0.428\n","[003/00109] val_loss: 0.347, val_accuracy: 87.361%, val_iou: 0.971\n","[003/00159] train_loss: 0.405\n","[003/00259] train_loss: 0.392\n","[003/00359] train_loss: 0.386\n","[003/00359] val_loss: 0.328, val_accuracy: 88.088%, val_iou: 0.972\n","[004/00079] train_loss: 0.390\n","[004/00179] train_loss: 0.368\n","[004/00229] val_loss: 0.349, val_accuracy: 87.357%, val_iou: 0.970\n","[004/00279] train_loss: 0.373\n","[004/00379] train_loss: 0.368\n","[005/00099] train_loss: 0.365\n","[005/00099] val_loss: 0.326, val_accuracy: 88.281%, val_iou: 0.972\n","[005/00199] train_loss: 0.345\n","[005/00299] train_loss: 0.340\n","[005/00349] val_loss: 0.318, val_accuracy: 88.737%, val_iou: 0.973\n","[006/00019] train_loss: 0.351\n","[006/00119] train_loss: 0.360\n","[006/00219] train_loss: 0.344\n","[006/00219] val_loss: 0.325, val_accuracy: 88.189%, val_iou: 0.973\n","[006/00319] train_loss: 0.344\n","[007/00039] train_loss: 0.355\n","[007/00089] val_loss: 0.287, val_accuracy: 89.633%, val_iou: 0.974\n","[007/00139] train_loss: 0.318\n","[007/00239] train_loss: 0.355\n","[007/00339] train_loss: 0.346\n","[007/00339] val_loss: 0.291, val_accuracy: 89.641%, val_iou: 0.974\n","[008/00059] train_loss: 0.323\n","[008/00159] train_loss: 0.335\n","[008/00209] val_loss: 0.289, val_accuracy: 89.344%, val_iou: 0.974\n","[008/00259] train_loss: 0.332\n","[008/00359] train_loss: 0.317\n","[009/00079] train_loss: 0.322\n","[009/00079] val_loss: 0.294, val_accuracy: 89.189%, val_iou: 0.974\n","[009/00179] train_loss: 0.317\n","[009/00279] train_loss: 0.314\n","[009/00329] val_loss: 0.296, val_accuracy: 89.575%, val_iou: 0.975\n","[009/00379] train_loss: 0.341\n","[010/00099] train_loss: 0.307\n","[010/00199] train_loss: 0.318\n","[010/00199] val_loss: 0.296, val_accuracy: 89.063%, val_iou: 0.974\n","[010/00299] train_loss: 0.317\n","[011/00019] train_loss: 0.334\n","[011/00069] val_loss: 0.270, val_accuracy: 90.058%, val_iou: 0.975\n","[011/00119] train_loss: 0.319\n","[011/00219] train_loss: 0.322\n","[011/00319] train_loss: 0.310\n","[011/00319] val_loss: 0.308, val_accuracy: 88.327%, val_iou: 0.972\n"]}],"source":["from exercise_2.training import train_pointnet_segmentation\n","config = {\n","    'experiment_name': '2_5_pointnet_segmentation_generalization',\n","    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n","    'is_overfit': False,\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate': 0.001,\n","    'max_epochs': 12,\n","    'print_every_n': 100,\n","    'validate_every_n': 250,\n","}\n","\n","train_pointnet_segmentation.main(config)  # Should be able to get > 90% accuracy and > 0.8 iou on the val set"]},{"cell_type":"markdown","metadata":{"id":"rDsrUTLcdsG2"},"source":["### (f) Inference using the trained model"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"Y7ips8BCdsG2"},"outputs":[],"source":["from exercise_2.inference.infer_pointnet_segmentation import InferenceHandlerPointNetSegmentation\n","from exercise_2.util.visualization import visualize_pointcloud\n","from matplotlib import cm, colors\n","import numpy as np\n","\n","# create a handler for inference using a trained checkpoint\n","inferer = InferenceHandlerPointNetSegmentation('exercise_2/runs/2_5_pointnet_segmentation_generalization/model_best.ckpt')"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"t1QLYRSLdsG3"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\TUMsemester 2\\Machine Learning for 3D Geometry\\Exercises\\.env\\lib\\site-packages\\traittypes\\traittypes.py:97: UserWarning: Given trait value dtype \"int32\" does not match required type \"uint32\". A coerced copy has been created.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8604b0b4796f411dbfeb03393acc87ec","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Get shape point cloud, predict labels, and visualize colored point cloud\n","shape_points = ShapeNetParts.get_point_cloud_with_labels('02691156/1c4b8662938adf41da2b0f839aba40f9')[0]\n","point_labels = inferer.infer_single(shape_points)\n","point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n","point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n","point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n","visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"L6lFbHCzdsG3"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7adef50612f84243b6f773f1a6275b6a","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Get shape point cloud, predict labels, and visualize colored point cloud\n","shape_points = ShapeNetParts.get_point_cloud_with_labels('03948459/e017cf5dac1e39b013d74211a209ce')[0]\n","point_labels = inferer.infer_single(shape_points)\n","point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n","point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n","point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n","visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"qN2ubwz6dsG4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"142ae197aef9460aa54c826be32b4b30","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Get shape point cloud, predict labels, and visualize colored point cloud\n","shape_points = ShapeNetParts.get_point_cloud_with_labels('03790512/86b6dc954e1ca8e948272812609617e2')[0]\n","point_labels = inferer.infer_single(shape_points)\n","point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n","point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n","point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n","visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"]},{"cell_type":"markdown","metadata":{"id":"n9QXUbHadsG4"},"source":["Make sure you submit the trained model exercise_2/runs/2_5_pointnet_segmentation_generalization/model_best.ckpt in your zip so that we can evaluate it on the test set at our end."]},{"cell_type":"markdown","metadata":{"id":"QZljKeMUdsG4"},"source":["## Submission\n","\n","This is the end of exercise 2 🙂. Please create a zip containing all files we provided, everything you modified, and all of your generated output/visualization files, including your checkpoints. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle.\n","\n","**Submission Deadline**: 23.11.2022 23:55"]},{"cell_type":"markdown","metadata":{"id":"3xY4CPR8dsG5"},"source":["## References\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lOCMeQ3hdsG6"},"source":["[1] Qi, C. et al. “Volumetric and Multi-view CNNs for Object Classification on 3D Data.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 5648-5656.\n","\n","[2] Qi, C. et al. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 77-85."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["XM41UsgzdsGa","JiYsCJPudsGc","5fZRmUsZdsGc","flnUF_avdsGe","sW15DcmLdsGf","lbkgBo1-dsGl","FywurCTcdsGm","wDGW9DBedsGo","fEONAuVadsGp","k5QN5pgMdsGq","7ZNpnBnodsGr","g3FO6moOdsGv","HjPfMvi8dsGx","gPz73wARdsGy","28zdfoz6dsGz","c8iZ0c4DdsG0","rDsrUTLcdsG2"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.0 ('.env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"f89730d4649948e1aa0590ffed258941240cc930c08639bca08a5709f929253a"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"02db40411a4c40c8a1ffa4ace12b60ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bcf1bdd64cc46988b06dee548a87c9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eece8487e41493289f1f8c6bd921431":{"model_module":"k3d","model_module_version":"2.14.5","model_name":"PlotModel","state":{"_backend_version":"2.14.5","_dom_classes":[],"_model_module":"k3d","_model_module_version":"2.14.5","_model_name":"PlotModel","_view_count":null,"_view_module":"k3d","_view_module_version":"2.14.5","_view_name":"PlotView","antialias":3,"auto_rendering":true,"axes":["x","y","z"],"axes_helper":1,"axes_helper_colors":[16711680,65280,255],"background_color":16777215,"camera":[],"camera_animation":[],"camera_auto_fit":true,"camera_damping_factor":0,"camera_fov":60,"camera_mode":"trackball","camera_no_pan":false,"camera_no_rotate":false,"camera_no_zoom":false,"camera_pan_speed":0.3,"camera_rotate_speed":1,"camera_zoom_speed":1.2,"clipping_planes":[],"colorbar_object_id":-1,"colorbar_scientific":false,"custom_data":null,"fps":25,"fps_meter":false,"grid":[-0.55,-0.55,-0.55,0.55,0.55,0.55],"grid_auto_fit":true,"grid_color":15132390,"grid_visible":false,"height":512,"label_color":4473924,"layout":"IPY_MODEL_eef1fb5f4a5c4b5fa418b04e0d031f36","lighting":1.5,"logarithmic_depth_buffer":true,"manipulate_mode":"translate","menu_visibility":true,"minimum_fps":20,"mode":"view","name":"occupancy_grid","object_ids":[140543185155408],"rendering_steps":1,"screenshot":"","screenshot_scale":2,"snapshot":"","snapshot_type":"full","time":0,"voxel_paint_color":0}},"21d33b16e37a4876b56e6066cad89915":{"model_module":"k3d","model_module_version":"2.14.5","model_name":"PlotModel","state":{"_backend_version":"2.14.5","_dom_classes":[],"_model_module":"k3d","_model_module_version":"2.14.5","_model_name":"PlotModel","_view_count":null,"_view_module":"k3d","_view_module_version":"2.14.5","_view_name":"PlotView","antialias":3,"auto_rendering":true,"axes":["x","y","z"],"axes_helper":1,"axes_helper_colors":[16711680,65280,255],"background_color":16777215,"camera":[],"camera_animation":[],"camera_auto_fit":true,"camera_damping_factor":0,"camera_fov":60,"camera_mode":"trackball","camera_no_pan":false,"camera_no_rotate":false,"camera_no_zoom":false,"camera_pan_speed":0.3,"camera_rotate_speed":1,"camera_zoom_speed":1.2,"clipping_planes":[],"colorbar_object_id":-1,"colorbar_scientific":false,"custom_data":null,"fps":25,"fps_meter":false,"grid":[-0.55,-0.55,-0.55,0.55,0.55,0.55],"grid_auto_fit":true,"grid_color":15132390,"grid_visible":false,"height":512,"label_color":4473924,"layout":"IPY_MODEL_1bcf1bdd64cc46988b06dee548a87c9c","lighting":1.5,"logarithmic_depth_buffer":true,"manipulate_mode":"translate","menu_visibility":true,"minimum_fps":20,"mode":"view","name":"occupancy_grid","object_ids":[140543185156432],"rendering_steps":1,"screenshot":"","screenshot_scale":2,"snapshot":"","snapshot_type":"full","time":0,"voxel_paint_color":0}},"27f66f0921e443f1a2bbf1a61db52c67":{"model_module":"k3d","model_module_version":"2.14.5","model_name":"PlotModel","state":{"_backend_version":"2.14.5","_dom_classes":[],"_model_module":"k3d","_model_module_version":"2.14.5","_model_name":"PlotModel","_view_count":null,"_view_module":"k3d","_view_module_version":"2.14.5","_view_name":"PlotView","antialias":3,"auto_rendering":true,"axes":["x","y","z"],"axes_helper":1,"axes_helper_colors":[16711680,65280,255],"background_color":16777215,"camera":[],"camera_animation":[],"camera_auto_fit":true,"camera_damping_factor":0,"camera_fov":60,"camera_mode":"trackball","camera_no_pan":false,"camera_no_rotate":false,"camera_no_zoom":false,"camera_pan_speed":0.3,"camera_rotate_speed":1,"camera_zoom_speed":1.2,"clipping_planes":[],"colorbar_object_id":-1,"colorbar_scientific":false,"custom_data":null,"fps":25,"fps_meter":false,"grid":[-0.55,-0.55,-0.55,0.55,0.55,0.55],"grid_auto_fit":true,"grid_color":15132390,"grid_visible":false,"height":512,"label_color":4473924,"layout":"IPY_MODEL_980fb31a97f34092b57c0d9e76e1ecf8","lighting":1.5,"logarithmic_depth_buffer":true,"manipulate_mode":"translate","menu_visibility":true,"minimum_fps":20,"mode":"view","name":"occupancy_grid","object_ids":[140543195818768],"rendering_steps":1,"screenshot":"","screenshot_scale":2,"snapshot":"","snapshot_type":"full","time":0,"voxel_paint_color":0}},"31bd18878d494ec380d910b75392cb8f":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_4b2f6fd1a85842afb5030a511fe8cbcc","msg_id":"","outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cee74d91cf134277b7341cf7e27d28a4","version_major":2,"version_minor":0},"text/plain":"Plot(antialias=3, axes=['x', 'y', 'z'], axes_helper=1.0, axes_helper_colors=[16711680, 65280, 255], background…"},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}},"output_type":"display_data"}]}},"38e52805a06741f8b105ed14c51e965b":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_fed0d65396834ecb8a252edc067f469f","msg_id":"","outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1eece8487e41493289f1f8c6bd921431","version_major":2,"version_minor":0},"text/plain":"Plot(antialias=3, axes=['x', 'y', 'z'], axes_helper=1.0, axes_helper_colors=[16711680, 65280, 255], background…"},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}},"output_type":"display_data"}]}},"42b279636ea04f39b740dac04d4eb495":{"model_module":"k3d","model_module_version":"2.14.5","model_name":"PlotModel","state":{"_backend_version":"2.14.5","_dom_classes":[],"_model_module":"k3d","_model_module_version":"2.14.5","_model_name":"PlotModel","_view_count":null,"_view_module":"k3d","_view_module_version":"2.14.5","_view_name":"PlotView","antialias":3,"auto_rendering":true,"axes":["x","y","z"],"axes_helper":1,"axes_helper_colors":[16711680,65280,255],"background_color":16777215,"camera":[],"camera_animation":[],"camera_auto_fit":true,"camera_damping_factor":0,"camera_fov":60,"camera_mode":"trackball","camera_no_pan":false,"camera_no_rotate":false,"camera_no_zoom":false,"camera_pan_speed":0.3,"camera_rotate_speed":1,"camera_zoom_speed":1.2,"clipping_planes":[],"colorbar_object_id":-1,"colorbar_scientific":false,"custom_data":null,"fps":25,"fps_meter":false,"grid":[-0.55,-0.55,-0.55,0.55,0.55,0.55],"grid_auto_fit":true,"grid_color":15132390,"grid_visible":false,"height":512,"label_color":4473924,"layout":"IPY_MODEL_d5ec128b778a4902b06e134fbb526d4e","lighting":1.5,"logarithmic_depth_buffer":true,"manipulate_mode":"translate","menu_visibility":true,"minimum_fps":20,"mode":"view","name":"occupancy_grid","object_ids":[140543184885328],"rendering_steps":1,"screenshot":"","screenshot_scale":2,"snapshot":"","snapshot_type":"full","time":0,"voxel_paint_color":0}},"4b2f6fd1a85842afb5030a511fe8cbcc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63217bb57c5344daa340ebd060fc095a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66f954ed799e470692b17c9f7bc345b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70c669c566ef46798964991bb8427b64":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_66f954ed799e470692b17c9f7bc345b3","msg_id":"","outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27f66f0921e443f1a2bbf1a61db52c67","version_major":2,"version_minor":0},"text/plain":"Plot(antialias=3, axes=['x', 'y', 'z'], axes_helper=1.0, axes_helper_colors=[16711680, 65280, 255], background…"},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}},"output_type":"display_data"}]}},"8719f752eb3e4ef3b9030dfb16b4a7c5":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_ad1e685c52e94d66879a519023fb66ee","msg_id":"","outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21d33b16e37a4876b56e6066cad89915","version_major":2,"version_minor":0},"text/plain":"Plot(antialias=3, axes=['x', 'y', 'z'], axes_helper=1.0, axes_helper_colors=[16711680, 65280, 255], background…"},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}},"output_type":"display_data"}]}},"910e9b0d4ad145d48f20e220c2f30041":{"model_module":"k3d","model_module_version":"2.14.5","model_name":"PlotModel","state":{"_backend_version":"2.14.5","_dom_classes":[],"_model_module":"k3d","_model_module_version":"2.14.5","_model_name":"PlotModel","_view_count":null,"_view_module":"k3d","_view_module_version":"2.14.5","_view_name":"PlotView","antialias":3,"auto_rendering":true,"axes":["x","y","z"],"axes_helper":1,"axes_helper_colors":[16711680,65280,255],"background_color":16777215,"camera":[],"camera_animation":[],"camera_auto_fit":true,"camera_damping_factor":0,"camera_fov":60,"camera_mode":"trackball","camera_no_pan":false,"camera_no_rotate":false,"camera_no_zoom":false,"camera_pan_speed":0.3,"camera_rotate_speed":1,"camera_zoom_speed":1.2,"clipping_planes":[],"colorbar_object_id":-1,"colorbar_scientific":false,"custom_data":null,"fps":25,"fps_meter":false,"grid":[-0.55,-0.55,-0.55,0.55,0.55,0.55],"grid_auto_fit":true,"grid_color":15132390,"grid_visible":false,"height":512,"label_color":4473924,"layout":"IPY_MODEL_dfeae9c358d646c38ff0c6cbe7e50d25","lighting":1.5,"logarithmic_depth_buffer":true,"manipulate_mode":"translate","menu_visibility":true,"minimum_fps":20,"mode":"view","name":"occupancy_grid","object_ids":[140543195784656],"rendering_steps":1,"screenshot":"","screenshot_scale":2,"snapshot":"","snapshot_type":"full","time":0,"voxel_paint_color":0}},"980fb31a97f34092b57c0d9e76e1ecf8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e7eafeae5e4f17a28407e2e383f6a4":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_63217bb57c5344daa340ebd060fc095a","msg_id":"","outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"910e9b0d4ad145d48f20e220c2f30041","version_major":2,"version_minor":0},"text/plain":"Plot(antialias=3, axes=['x', 'y', 'z'], axes_helper=1.0, axes_helper_colors=[16711680, 65280, 255], background…"},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}},"output_type":"display_data"}]}},"ad1e685c52e94d66879a519023fb66ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce7a9238aa5c47d3ba7b0a4533c810b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cee74d91cf134277b7341cf7e27d28a4":{"model_module":"k3d","model_module_version":"2.14.5","model_name":"PlotModel","state":{"_backend_version":"2.14.5","_dom_classes":[],"_model_module":"k3d","_model_module_version":"2.14.5","_model_name":"PlotModel","_view_count":null,"_view_module":"k3d","_view_module_version":"2.14.5","_view_name":"PlotView","antialias":3,"auto_rendering":true,"axes":["x","y","z"],"axes_helper":1,"axes_helper_colors":[16711680,65280,255],"background_color":16777215,"camera":[],"camera_animation":[],"camera_auto_fit":true,"camera_damping_factor":0,"camera_fov":60,"camera_mode":"trackball","camera_no_pan":false,"camera_no_rotate":false,"camera_no_zoom":false,"camera_pan_speed":0.3,"camera_rotate_speed":1,"camera_zoom_speed":1.2,"clipping_planes":[],"colorbar_object_id":-1,"colorbar_scientific":false,"custom_data":null,"fps":25,"fps_meter":false,"grid":[-0.55,-0.55,-0.55,0.55,0.55,0.55],"grid_auto_fit":true,"grid_color":15132390,"grid_visible":false,"height":512,"label_color":4473924,"layout":"IPY_MODEL_ce7a9238aa5c47d3ba7b0a4533c810b6","lighting":1.5,"logarithmic_depth_buffer":true,"manipulate_mode":"translate","menu_visibility":true,"minimum_fps":20,"mode":"view","name":"point_cloud","object_ids":[140543192409936],"rendering_steps":1,"screenshot":"","screenshot_scale":2,"snapshot":"","snapshot_type":"full","time":0,"voxel_paint_color":0}},"d5ec128b778a4902b06e134fbb526d4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfeae9c358d646c38ff0c6cbe7e50d25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eef1fb5f4a5c4b5fa418b04e0d031f36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f154013bd98144ea8641c8430215cb1a":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_02db40411a4c40c8a1ffa4ace12b60ab","msg_id":"","outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42b279636ea04f39b740dac04d4eb495","version_major":2,"version_minor":0},"text/plain":"Plot(antialias=3, axes=['x', 'y', 'z'], axes_helper=1.0, axes_helper_colors=[16711680, 65280, 255], background…"},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}},"output_type":"display_data"}]}},"fed0d65396834ecb8a252edc067f469f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
